{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"118w-se0rELv-jbdnzbvPDhgzviBg3hmy","timestamp":1727988193309},{"file_id":"1gzJrRXgM5UnrQ-0TcDORLh-nf9mPe7e3","timestamp":1608145618791},{"file_id":"1NEuWUdI_RE21-HSnM97Gaiu7r0mgOpzS","timestamp":1575908111833},{"file_id":"1W6yojd2fpYsDP2mnH_pbyFF8DzormXTA","timestamp":1575458798048},{"file_id":"1MhBkPZTpU52Jf2Y50ZAMG0kO6e4mS3Il","timestamp":1574780203309},{"file_id":"1nWQYnWfLw5xvcASlIHosaJWbXPhDIEG1","timestamp":1574515967280}],"machine_shape":"hm","toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"cells":[{"cell_type":"markdown","metadata":{"id":"XMMppWbnG3dN"},"source":["# Préambule\n","\n","On se propose dans ce TP d'illustrer certains techniques **d'apprentissage semi-supervisé** vues en cours.\n","\n","Dans tout ce qui suit, on considère que l'on dispose d'un ensemble de données $x_{lab}$ labellisées et d'un ensemble de donnés $x_{unlab}$ non labellisées."]},{"cell_type":"markdown","metadata":{"id":"2onzaW7mJrgG"},"source":["## Datasets"]},{"cell_type":"markdown","source":["Commencez par exécuter ces codes qui vos permettront de charger les datasets que nous allons utiliser et de les partager en données labellisées et non labellisées, ainsi qu'en données de test."],"metadata":{"id":"4BejOODdKZ70"}},{"cell_type":"markdown","metadata":{"id":"V2nYQ2X5JW2k"},"source":["### Dataset des deux clusters"]},{"cell_type":"code","metadata":{"id":"Pkv-k9qIJyXH"},"source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn import datasets\n","import matplotlib.pyplot as plt\n","\n","def generate_2clusters_dataset(num_lab = 10, num_unlab=740, num_test=250):\n","  num_samples = num_lab + num_unlab + num_test\n","  # Génération de 1000 données du dataset des 2 lunes\n","  x, y = datasets.make_blobs(n_samples=[round(num_samples/2), round(num_samples/2)], n_features=2, center_box=(- 3, 3), random_state=1)\n","\n","  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=num_test/num_samples, random_state=1)\n","  x_train_lab, x_train_unlab, y_train_lab, y_train_unlab = train_test_split(x_train, y_train, test_size=num_unlab/(num_unlab+num_lab), random_state=6)\n","\n","  return x_train_lab, y_train_lab, x_train_unlab, y_train_unlab, x_test, y_test\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OBwkuDKFLKdH"},"source":["x_train_lab, y_train_lab, x_train_unlab, y_train_unlab, x_test, y_test = generate_2clusters_dataset(num_lab = 10, num_unlab=740, num_test=250)\n","\n","print(x_train_lab.shape, x_train_unlab.shape, x_test.shape)\n","print(y_train_lab.shape, y_train_unlab.shape, y_test.shape)\n","\n","# Affichage des données\n","plt.plot(x_train_unlab[y_train_unlab==0,0], x_train_unlab[y_train_unlab==0,1], 'b.')\n","plt.plot(x_train_unlab[y_train_unlab==1,0], x_train_unlab[y_train_unlab==1,1], 'r.')\n","\n","plt.plot(x_test[y_test==0,0], x_test[y_test==0,1], 'b+')\n","plt.plot(x_test[y_test==1,0], x_test[y_test==1,1], 'r+')\n","\n","plt.plot(x_train_lab[y_train_lab==0,0], x_train_lab[y_train_lab==0,1], 'b.', markersize=30)\n","plt.plot(x_train_lab[y_train_lab==1,0], x_train_lab[y_train_lab==1,1], 'r.', markersize=30)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sKR9vNgsLp_J"},"source":["### Dataset des 2 lunes"]},{"cell_type":"markdown","metadata":{"id":"_AFhsTUQwIxt"},"source":["\n","<img src=\"https://drive.google.com/uc?id=1xb_gasBJ6sEmbyvCWTnVEAsbspyDCyFL\">\n","<caption><center> Figure 1: Comparaison de différents algorithmes semi-supervisés sur le dataset des 2 lunes</center></caption>"]},{"cell_type":"code","metadata":{"id":"tCw5v2JDLwau"},"source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn import datasets\n","import matplotlib.pyplot as plt\n","\n","def generate_2moons_dataset(num_lab = 10, num_unlab=740, num_test=250):\n","  num_samples = num_lab + num_unlab + num_test\n","  # Génération de 1000 données du dataset des 2 lunes\n","  x, y = datasets.make_moons(n_samples=num_samples, noise=0.1, random_state=1)\n","\n","  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=num_test/num_samples, random_state=1)\n","  x_train_lab, x_train_unlab, y_train_lab, y_train_unlab = train_test_split(x_train, y_train, test_size=num_unlab/(num_unlab+num_lab), random_state=6)\n","\n","  return x_train_lab, y_train_lab, x_train_unlab, y_train_unlab, x_test, y_test\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FkQ1L5I1MBkH"},"source":["x_train_lab, y_train_lab, x_train_unlab, y_train_unlab, x_test, y_test = generate_2moons_dataset(num_lab = 10, num_unlab=740, num_test=250)\n","\n","print(x_train_lab.shape, x_train_unlab.shape, x_test.shape)\n","print(y_train_lab.shape, y_train_unlab.shape, y_test.shape)\n","\n","# Affichage des données\n","plt.plot(x_train_unlab[y_train_unlab==0,0], x_train_unlab[y_train_unlab==0,1], 'b.')\n","plt.plot(x_train_unlab[y_train_unlab==1,0], x_train_unlab[y_train_unlab==1,1], 'r.')\n","\n","plt.plot(x_test[y_test==0,0], x_test[y_test==0,1], 'b+')\n","plt.plot(x_test[y_test==1,0], x_test[y_test==1,1], 'r+')\n","\n","plt.plot(x_train_lab[y_train_lab==0,0], x_train_lab[y_train_lab==0,1], 'b.', markersize=30)\n","plt.plot(x_train_lab[y_train_lab==1,0], x_train_lab[y_train_lab==1,1], 'r.', markersize=30)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kH_eMruIMMVF"},"source":["### MNIST\n"]},{"cell_type":"code","metadata":{"id":"ebv6WLB1MOkU"},"source":["from tensorflow.keras.datasets import mnist\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","def generate_mnist_dataset(num_lab = 10):\n","\n","  # Chargement et normalisation (entre 0 et 1) des données de la base de données MNIST\n","  (x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","  x_train = np.expand_dims(x_train.astype('float32') / 255., 3)\n","  x_test = np.expand_dims(x_test.astype('float32') / 255., 3)\n","\n","  x_train_lab, x_train_unlab, y_train_lab, y_train_unlab = train_test_split(x_train, y_train, test_size=(x_train.shape[0]-num_lab)/x_train.shape[0], random_state=2)\n","\n","  return x_train_lab, y_train_lab, x_train_unlab, y_train_unlab, x_test, y_test\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZTsEZ2pzMpiU"},"source":["x_train_lab, y_train_lab, x_train_unlab, y_train_unlab, x_test, y_test = generate_mnist_dataset(num_lab = 10)\n","\n","print(x_train_lab.shape, x_train_unlab.shape, x_test.shape)\n","print(y_train_lab.shape, y_train_unlab.shape, y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NIGZe-yAQq-A"},"source":["## Modèles"]},{"cell_type":"markdown","source":["Nous allons dès maintenant préparer les modèles que nous utiliserons dans la suite, à vous de les compléter :"],"metadata":{"id":"jaPezVmtK5tC"}},{"cell_type":"code","metadata":{"id":"BryV3CDKQytA"},"source":["from tensorflow.keras.layers import *\n","from tensorflow.keras import Model, Input\n","# A COMPLETER\n","# Ici, écrire un simple perceptron monocouche\n","def create_model_2clusters():\n","\n","  inputs = Input(shape=(...))\n","\n","  outputs = ...\n","\n","  model = Model(inputs=inputs, outputs=outputs)\n","\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o1jcG_4pyGlx"},"source":["# A COMPLETER\n","# Ici, écrire un perceptron multi-couches à une seule couche cachée comprenant 20 neurones\n","def create_model_2moons():\n","\n","  inputs = keras.Input(shape=(...))\n","\n","\n","\n","  outputs = ...\n","  model = keras.Model(inputs=inputs, outputs=outputs)\n","\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A COMPLETER\n","# Ici, on implémentera le modèle LeNet-5 :\n","# 1 couche de convolution 5x5 à 6 filtres suivie d'un max pooling\n","# puis 1 couche de convolution 5x5 à 16 filtres suivie d'un max pooling et d'un Flatten\n","# Enfin 2 couches denses de 120 et 84 neurones, avant la couche de sortie à 10 neurones.\n","def create_model_mnist():\n","\n","  inputs = keras.Input(shape=(...))\n","\n","\n","  outputs = ...\n","\n","  model = keras.Model(inputs=inputs, outputs=outputs)\n","\n","  return model"],"metadata":{"id":"TklMVuuvLePW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JMaTgZJcQbIh"},"source":["# Apprentissage supervisé"]},{"cell_type":"markdown","source":["Commencez par bien lire le code ci-dessous, qui vous permet de mettre en place un apprentissage supervisé en détaillant la boucle d'apprentissage. Cela nous permettra d'avoir plus de contrôle dans la suite pour implémenter les algorithmes semi-supervisés.\n","\n","Cela vous fournira également une base contre laquelle comparer les algorithmes semi-supervisés."],"metadata":{"id":"hGTfv5YfMAXY"}},{"cell_type":"markdown","metadata":{"id":"fbmhai8PVXVd"},"source":["### Dataset des 2 clusters"]},{"cell_type":"code","metadata":{"id":"XP5XgJRQQm5_"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","import math\n","\n","# Données et modèle du problème des 2 clusters\n","x_train_lab, y_train_lab, x_train_unlab, y_train_unlab, x_test, y_test = generate_2clusters_dataset(num_lab = 10, num_unlab=740, num_test=250)\n","model = create_model_2clusters()\n","\n","# Hyperparamètres de l'apprentissage\n","epochs = 150\n","batch_size = 16\n","if batch_size < x_train_lab.shape[0]:\n","  steps_per_epoch = math.floor(x_train_lab.shape[0]/batch_size)\n","else:\n","  steps_per_epoch = 1\n","  batch_size = x_train_lab.shape[0]\n","\n","# Instanciation d'un optimiseur et d'une fonction de coût.\n","optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n","loss_fn = keras.losses.BinaryCrossentropy()\n","\n","# Préparation des métriques pour le suivi de la performance du modèle.\n","train_acc_metric = keras.metrics.BinaryAccuracy()\n","test_acc_metric = keras.metrics.BinaryAccuracy()\n","\n","# Indices de l'ensemble labellisé\n","indices = np.arange(x_train_lab.shape[0])\n","\n","# Boucle sur les epochs\n","for epoch in range(epochs):\n","\n","  # A chaque nouvelle epoch, on randomise les indices de l'ensemble labellisé\n","  np.random.shuffle(indices)\n","\n","  # Et on recommence à cumuler la loss\n","  cum_loss_value = 0\n","\n","  for step in range(steps_per_epoch):\n","\n","    # Sélection des données du prochain batch\n","    x_batch = x_train_lab[indices[step*batch_size: (step+1)*batch_size]]\n","    y_batch = y_train_lab[indices[step*batch_size: (step+1)*batch_size]]\n","\n","    # Etape nécessaire pour comparer y_batch à la sortie du réseau\n","    y_batch = np.expand_dims(y_batch, 1)\n","\n","    # Les opérations effectuées par le modèle dans ce bloc sont suivies et permettront\n","    # la différentiation automatique.\n","    with tf.GradientTape() as tape:\n","\n","      # Application du réseau aux données d'entrée\n","      y_pred = model(x_batch, training=True)  # Logits for this minibatch\n","\n","      # Calcul de la fonction de perte sur ce batch\n","      loss_value = loss_fn(y_batch, y_pred)\n","\n","      # Calcul des gradients par différentiation automatique\n","      grads = tape.gradient(loss_value, model.trainable_weights)\n","\n","      # Réalisation d'une itération de la descente de gradient (mise à jour des paramètres du réseau)\n","      optimizer.apply_gradients(zip(grads, model.trainable_weights))\n","\n","      # Mise à jour de la métrique\n","      train_acc_metric.update_state(y_batch, y_pred)\n","\n","      cum_loss_value = cum_loss_value + loss_value\n","\n","  # Calcul de la précision à la fin de l'epoch\n","  train_acc = train_acc_metric.result()\n","\n","  # Calcul de la précision sur l'ensemble de test à la fin de l'epoch\n","  test_logits = model(x_test, training=False)\n","  test_acc_metric.update_state(np.expand_dims(y_test, 1), test_logits)\n","  test_acc = test_acc_metric.result()\n","\n","  print(\"Epoch %4d : Loss : %.4f, Acc : %.4f, Test Acc : %.4f\" % (epoch, float(cum_loss_value/steps_per_epoch), float(train_acc), float(test_acc)))\n","\n","  # Remise à zéro des métriques pour la prochaine epoch\n","  train_acc_metric.reset_state()\n","  test_acc_metric.reset_state()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FcnTF5WWVacl"},"source":["from mlxtend.plotting import plot_decision_regions\n","\n","# Affichage des données\n","plt.plot(x_train_unlab[y_train_unlab==0,0], x_train_unlab[y_train_unlab==0,1], 'b.')\n","plt.plot(x_train_unlab[y_train_unlab==1,0], x_train_unlab[y_train_unlab==1,1], 'r.')\n","\n","plt.plot(x_test[y_test==0,0], x_test[y_test==0,1], 'b+')\n","plt.plot(x_test[y_test==1,0], x_test[y_test==1,1], 'r+')\n","\n","plt.plot(x_train_lab[y_train_lab==0,0], x_train_lab[y_train_lab==0,1], 'b.', markersize=30)\n","plt.plot(x_train_lab[y_train_lab==1,0], x_train_lab[y_train_lab==1,1], 'r.', markersize=30)\n","\n","plt.show()\n","\n","#Affichage de la frontière de décision\n","plot_decision_regions(x_train_unlab, y_train_unlab, clf=model, legend=2)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"caF0geTEx5Zv"},"source":["### Dataset des 2 lunes"]},{"cell_type":"markdown","source":["Remettez en place le même apprentissage pour le dataset des 2 lunes."],"metadata":{"id":"GdFbVzKbMcYE"}},{"cell_type":"markdown","metadata":{"id":"YPiuBS36V8EG"},"source":["# Minimisation de l'entropie"]},{"cell_type":"markdown","source":["Nous allons dans cette partie mettre en place le mécanisme de minimisation d'entropie, conjointement à la minimisation de l'entropie croisée.\n","\n","Pour commencer, implémentez la fonction de coût qui calcule l'entropie $H$ des prédictions du réseau $\\hat{y}$ :\n","$$ H(\\hat{y}) = - ∑_{i=1}^N \\hat{y}_i log(\\hat{y}_i) $$\n","\n","(pour éviter les cas dégénérés, je vous conseille de pre-process les valeurs de y_pred pour qu'elles se situent dans un intervalle $[\\epsilon, 1-\\epsilon]$ grâce à la fonction *tf.clip_by_value*)"],"metadata":{"id":"UlBFMsFLMtEp"}},{"cell_type":"code","metadata":{"id":"1gEt2x_sXFin"},"source":["import tensorflow as tf\n","\n","# Calcul de l'entropie de y_pred\n","# A COMPLETER\n","def entropy_loss(y_pred):\n","    return ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Travail à faire** : Reprenez maintenant la boucle d'apprentissage supervisé et introduisez la minimisation d'entropie pour régulariser l'apprentissage.\n","\n","La difficulté principale va être l'introduction des données non labellisées dans la boucle. Ainsi, un batch devra maintenant être composé de données labellisées et non labellisées. Je vous suggère de conserver le même nombre de données labellisées par batch que précédemment (i.e. 16) et de prendre un plus grand nombre de données non labellisées, par exemple 90.\n","\n","N'oubliez pas également d'introduire un hyperparamètre λ pour contrôler l'équilibre entre perte supervisée et non supervisée. Utilisez un λ constant dans un premier temps, et testez ensuite des variantes qui consisteraient à augmenter progressivement sa valeur au fil des epochs.\n","\n","La fonction objectif à minimiser aura donc la forme :    \n","$$  J = \\sum_{(x,y) \\in \\mathcal{L}} CE(y, \\hat{y}) + \\lambda \\sum_{x \\in \\mathcal{U}} H(\\hat{y})\t$$"],"metadata":{"id":"-L1Li1YtNN87"}},{"cell_type":"code","source":[],"metadata":{"id":"nV5r5WPEIIPi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# $\\Pi$-Modèle\n","\n","Nous allons maintenant tenter d'utiliser un 2nd algorithme semi-supervisé supposé être plus efficace, il s'agit de l'algorithme du $\\Pi$-Modèle, donc la version détaillée est présentée ci-dessous (en VO).\n","\n","<img src=\"https://drive.google.com/uc?id=13VhlBYwA6YIYGzKI81Jom_jTiuhOypEg\">\n","<caption><center> Figure 2 : Pseudo-code de l'algorithme du $\\Pi$-Modèle</center></caption>\n"],"metadata":{"id":"bJEZBm9wIOym"}},{"cell_type":"markdown","source":["Ci-dessous, la boucle d'entraînement détaillée est reprise et contient un squelette du code à réaliser pour implémenter le $\\Pi$-Modèle.\n","\n","**Travail à faire :** Complétez le squelette de l'algorithme du $\\Pi$-Modèle pour pouvoir tester ce nouvel algorithme."],"metadata":{"id":"87lDYcvuIgqc"}},{"cell_type":"code","source":["# Nombre d'epochs de l'apprentissage\n","epochs = 2000\n","# Nombre de données non-labellisées par batch\n","bs_unlab = 100\n","# Nombre de données labellisées par batch\n","bs_lab = 10\n","# Taille du batch\n","batch_size = bs_lab + bs_unlab\n","# Valeur initiale du paramètre de contrôle de l'importance de la régularisation non-supervisée\n","lambda_t = 0\n","\n","# Données et modèle du problème des 2 clusters\n","x_train_lab, y_train_lab, x_train_unlab, y_train_unlab, x_test, y_test = generate_2moons_dataset(num_lab = 10, num_unlab=740, num_test=250)\n","model = create_model_2moons()\n","\n","# Nombre de batches par epochs\n","steps_per_epochs = int(np.floor(x_train_lab.shape[0]/bs_lab))\n","# Instanciation d'un optimiseur et d'une fonction de coût.\n","optimizer = keras.optimizers.Adam(learning_rate=3e-2)\n","# ICI ON A BESOIN DE DEUX FONCTIONS DE COUT :\n","# L'une pour la partie supervisée de la perte\n","loss_sup = ...\n","# L'autre pour la partie non-supervisée de la perte\n","loss_unsup =  ...\n","\n","# Préparation des métriques pour le suivi de la performance du modèle.\n","train_acc_metric = keras.metrics.BinaryAccuracy()\n","val_acc_metric = keras.metrics.BinaryAccuracy()\n","\n","# Indices de l'ensemble non labellisé\n","indices_lab = np.arange(x_train_lab.shape[0])\n","# Indices de l'ensemble non labellisé\n","indices_unlab = np.arange(x_train_unlab.shape[0])\n","\n","for epoch in range(epochs):\n","\n","  # A chaque nouvelle epoch, on randomise les indices des ensembles\n","  np.random.shuffle(indices_lab)\n","  np.random.shuffle(indices_unlab)\n","\n","  for b in range(steps_per_epochs):\n","\n","    # Les données d'un batch sont constituées de l'intégralité de nos données labellisées...\n","    x_batch_lab = x_train_lab[indices_lab[b*bs_lab:(b+1)*bs_lab]]\n","    y_batch_lab = y_train_lab[indices_lab[b*bs_lab:(b+1)*bs_lab]]\n","    # ... ainsi que de données non-labellisées !\n","    x_batch_unlab = x_train_unlab[indices_unlab[b*bs_unlab:(b+1)*bs_unlab]]\n","\n","    # On forme notre batch en concaténant les données labellisées et non labellisées\n","    x_batch = np.concatenate((x_batch_lab, x_batch_unlab), axis=0)\n","\n","    # On forme également un batch alternatif constitué des mêmes données bruitées\n","    # Le bruit ici sera simplement obtenu avec np.randn()\n","    # Attention à l'échelle du bruit !\n","    x_batch_noisy = ...\n","\n","    # Les opérations effectuées par le modèle dans ce bloc sont suivies et permettront\n","    # la différentiation automatique.\n","    with tf.GradientTape() as tape:\n","\n","      # Application du réseau aux données d'entrée\n","      y_pred = model(x_batch, training=True)\n","      # Ne pas oublier de le faire également sur le 2e batch !\n","      y_pred_noisy = model(x_batch_noisy, training=True)\n","\n","      # Calcul de la fonction de perte sur ce batch\n","      sup_term = ...\n","      unsup_term = ...\n","\n","      loss_value = ...\n","\n","      # Calcul des gradients par différentiation automatique\n","      grads = tape.gradient(loss_value, model.trainable_weights)\n","\n","      # Réalisation d'une itération de la descente de gradient (mise à jour des paramètres du réseau)\n","      optimizer.apply_gradients(zip(grads, model.trainable_weights))\n","\n","      # Mise à jour de la métrique\n","      train_acc_metric.update_state(np.expand_dims(y_batch_lab, 1), y_pred[0:bs_lab])\n","\n","\n","  # Calcul de la précision à la fin de l'epoch\n","  train_acc = train_acc_metric.result()\n","  # Calcul de la précision sur l'ensemble de validation à la fin de l'epoch\n","  val_logits = model(x_test, training=False)\n","  val_acc_metric.update_state(np.expand_dims(y_test, 1), val_logits)\n","  val_acc = val_acc_metric.result()\n","\n","  print(\"Epoch %4d : Loss : %.4f, Acc : %.4f, Val Acc : %.4f\" % (epoch, float(loss_value), float(train_acc), float(val_acc)))\n","\n","  # Remise à zéro des métriques pour la prochaine epoch\n","  train_acc_metric.reset_state()\n","  val_acc_metric.reset_state()\n","\n","  # Mise à jour du paramètre de contrôle de l'importance de la régularisation non-supervisée\n","  # Il augmente progressivement !\n","  if lambda_t < 1:\n","    if epoch > 100:\n","      lambda_t = lambda_t + 0.01\n","\n","\n"],"metadata":{"id":"rk_7nisfIPms"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# MNIST"],"metadata":{"id":"pr4EsmIxIopQ"}},{"cell_type":"markdown","source":["Pour adapter l'algorithme du $\\Pi$-modèle à MNIST, nous allons devoir remplacer le bruitage des données par de l'augmentation de données.\n","\n","Commencez par remplir l'ImageDataGenerator (à vous de voir la doc) avec des transformations pertinentes. **Attention** cette étape est cruciale pour obtenir de bons résultats. Il faut intégrer les augmentations les plus fortes possibles, mais être certain qu'elles ne modifient pas le label du chiffre !"],"metadata":{"id":"al8hGorPIuFd"}},{"cell_type":"code","source":["from tensorflow.keras.datasets import mnist\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","def generate_mnist_dataset(num_lab = 100):\n","\n","  # Chargement et normalisation (entre 0 et 1) des données de la base de données MNIST\n","  (x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","  x_train = np.expand_dims(x_train.astype('float32') / 255., 3)\n","  x_test = np.expand_dims(x_test.astype('float32') / 255., 3)\n","\n","  x_train_lab, x_train_unlab, y_train_lab, y_train_unlab = train_test_split(x_train, y_train, test_size=(x_train.shape[0]-num_lab)/x_train.shape[0], random_state=2)\n","\n","  return x_train_lab, y_train_lab, x_train_unlab, y_train_unlab, x_test, y_test\n","\n","x_train_lab, y_train_lab, x_train_unlab, y_train_unlab, x_test, y_test = generate_mnist_dataset()"],"metadata":{"id":"R5aS7XyLIqzz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import matplotlib.pyplot as plt\n","\n","train_datagen = ImageDataGenerator(\n","    # A COMPLETER\n","    ...)\n","\n","x = x_train_lab[0:10]\n","plt.imshow(x[0, : ,: ,0])\n","plt.show()\n","x_aug = train_datagen.flow(x, shuffle=False, batch_size=10).next()\n","plt.imshow(x_aug[0, : ,: ,0])\n","plt.show()"],"metadata":{"id":"x5nP5ta3IyFS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Travail à faire :** Reprenez maintenant le squelette précédent pour l'adapter à MNIST, en intégrant l'augmentation (à la place du bruitage des données). N'oubliez pas également de modifier les fonctions de coût !"],"metadata":{"id":"XSU_FURRI2sB"}}]}