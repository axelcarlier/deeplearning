{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"provenance":[{"file_id":"1gzJrRXgM5UnrQ-0TcDORLh-nf9mPe7e3","timestamp":1608145618791},{"file_id":"1NEuWUdI_RE21-HSnM97Gaiu7r0mgOpzS","timestamp":1575908111833},{"file_id":"1W6yojd2fpYsDP2mnH_pbyFF8DzormXTA","timestamp":1575458798048},{"file_id":"1MhBkPZTpU52Jf2Y50ZAMG0kO6e4mS3Il","timestamp":1574780203309},{"file_id":"1nWQYnWfLw5xvcASlIHosaJWbXPhDIEG1","timestamp":1574515967280}],"collapsed_sections":["B6dM--9H_wEM"],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"XMMppWbnG3dN"},"source":["## Auto-encodeurs\n","\n","L'objectif de ce TP est de manipuler des auto-encodeurs sur un exemple simple : la base de données MNIST. L'idée est de pouvoir visualiser les concepts vus en cours, et notamment d'illustrer la notion d'espace latent.\n","\n","Pour rappel, vous avez déjà manipulé les données MNIST en Analyse de Données en première année. Les images MNIST sont des images en niveaux de gris, de taille 28x28 pixels, représentant des chiffres manuscrits de 0 à 9.\n","\n","![mnist](http://i.ytimg.com/vi/0QI3xgXuB-Q/hqdefault.jpg)\n","\n","Pour démarrer, nous vous fournissons un code permettant de créer un premier auto-encodeur simple, de charger les données MNIST  et d'entraîner cet auto-encodeur. **L'autoencodeur n'est pas convolutif !** (libre à vous de le transformer pour qu'il le soit, plus tard dans le TP)"]},{"cell_type":"code","metadata":{"id":"nndrdDrlSkho"},"source":["from keras.layers import Input, Dense\n","from keras.models import Model\n","\n","# Dimension de l'entrée\n","input_img = Input(shape=(784,))\n","# Dimension de l'espace latent : PARAMETRE A TESTER !!\n","latent_dim = 32\n","\n","# Définition d'un encodeur\n","x = Dense(128, activation='relu')(input_img)\n","encoded = Dense(latent_dim, activation='linear')(x)\n","\n","# Définition d'un decodeur\n","decoder_input = Input(shape=(latent_dim,))\n","x = Dense(128, activation='relu')(decoder_input)\n","decoded = Dense(784, activation='linear')(x)\n","\n","# Construction d'un modèle séparé pour pouvoir accéder aux décodeur et encodeur\n","encoder = Model(input_img, encoded)\n","decoder = Model(decoder_input, decoded)\n","\n","# Construction du modèle de l'auto-encodeur\n","encoded = encoder(input_img)\n","decoded = decoder(encoded)\n","autoencoder = Model(input_img, decoded)\n","\n","autoencoder.compile(optimizer='Adam', loss='mse')\n","autoencoder.summary()\n","print(encoder.summary())\n","print(decoder.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jwYr0GHCSlIq"},"source":["from keras.datasets import mnist\n","import numpy as np\n","\n","# Chargement et normalisation (entre 0 et 1) des données de la base de données MNIST\n","(x_train, _), (x_test, _) = mnist.load_data()\n","\n","x_train = x_train.astype('float32') / 255.\n","x_test = x_test.astype('float32') / 255.\n","\n","# Vectorisation des images d'entrée en vecteurs de dimension 784\n","x_train = np.reshape(x_train, (len(x_train), 784))\n","x_test = np.reshape(x_test, (len(x_test), 784))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eKPiA41bSvxC"},"source":["# Entraînement de l'auto-encodeur. On utilise ici les données de test\n","# pour surveiller l'évolution de l'erreur de reconstruction sur des données\n","# non utilisées pendant l'entraînement et ainsi détecter le sur-apprentissage.\n","autoencoder.fit(x_train, x_train,\n","                epochs=50,\n","                batch_size=128,\n","                shuffle=True,\n","                validation_data=(x_test, x_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ohexDvCYrahC"},"source":["Le code suivant affiche des exemples d'images de la base de test (1e ligne) et de leur reconstruction (2e ligne)."]},{"cell_type":"code","metadata":{"id":"2SC9R1TRTUgN"},"source":["import matplotlib.pyplot as plt\n","\n","# Prédiction des données de test\n","decoded_imgs = autoencoder.predict(x_test)\n","\n","\n","n = 10\n","plt.figure(figsize=(20, 4))\n","for i in range(n):\n","    # Affichage de l'image originale\n","    ax = plt.subplot(2, n, i+1)\n","    plt.imshow(x_test[i].reshape(28, 28))\n","    plt.gray()\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","\n","    # Affichage de l'image reconstruite par l'auto-encodeur\n","    ax = plt.subplot(2, n, i+1 + n)\n","    plt.imshow(decoded_imgs[i].reshape(28, 28))\n","    plt.gray()\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Travail à faire"],"metadata":{"id":"4oV4eKk4p4Eg"}},{"cell_type":"markdown","metadata":{"id":"zjMnHWNtrgwZ"},"source":["Travail à faire\n","\n","1.   Pour commencer, observez les résultats obtenus avec le code fourni. Les résultats semblent imparfaits, les images reconstruites sont bruitées. Modifiez le code fourni pour transformer le problème de régression en classification binaire. Les résultats devraient être bien meilleurs ! Conservez cette formulation (même si elle est non standard) pour la suite.\n","2.   Avec la dimension d'espace latent qui vous est fournie, on observe une (relativement) faible erreur de reconstruction. Tracez une **courbe** (avec seulement quelques points) qui montre l'évolution de l'**erreur de reconstruction en fonction de la dimension de l'espace latent**. Quelle semble être la dimension minimale de l'espace latent qui permet encore d'observer une reconstruction raisonnable des données (avec le réseau qui vous est fourni) ?\n","3.   Pour diminuer encore plus la dimension de l'espace latent, il est nécessaire d'augmenter la capacité des réseaux encodeur et décodeur. Cherchez à nouveau la dimension minimale de l'espace latent qui permet d'observer une bonne reconstruction des données, mais en augmentant à l'envi la capacité de votre auto-encodeur.\n","4.   Écrivez une fonction qui, étant donné deux images de votre espace de test $I_1$ et $I_2$, réalise l'interpolation (avec, par exemple, 10 étapes) entre la représentation latente ($z_1 = $encoder($I_1$) et $z_2 = $encoder($I_2$)) de ces deux données, et génère les images $I_i$ correspondant aux représentations latentes intermédiaires $z_i$. En pseudo python, cela donne :\n","\n","```python\n","for i in range(10):\n","  z_i = z1 + i*(z2-z1)/10\n","  I_i = decoder(z_i)\n","```\n","Testez cette fonction avec un auto-encodeur avec une faible erreur de reconstruction, sur deux données présentant le même chiffre écrit différemment, puis deux chiffres différents.\n","5.   Pour finir, le code qui vous est fourni dans la suite permet de télécharger et de préparer une [base de données de visages](http://vis-www.cs.umass.edu/lfw/). ATTENTION : ici les images sont de taille $32\\times32$, en couleur, et comportent donc 3 canaux (contrairement aux images de MNIST, qui n'en comptent qu'un). Par analogie avec la question précédente, on pourrait grâce à la représentation latente apprise par un auto-encodeur, réaliser un morphing entre deux visages. Essayez d'abord d'entraîner un auto-encodeur à obtenir une erreur de reconstruction faible. Qu'observe-t-on ?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"B6dM--9H_wEM"},"source":["\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","# Visages"]},{"cell_type":"code","metadata":{"id":"Ot-zkfDBQUkl"},"source":["import pandas as pd\n","import tarfile, tqdm, cv2, os\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","# Télécharger les données de la base de données \"Labelled Faces in the Wild\"\n","!wget http://www.cs.columbia.edu/CAVE/databases/pubfig/download/lfw_attributes.txt\n","!wget http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz\n","!wget http://vis-www.cs.umass.edu/lfw/lfw.tgz\n","\n","ATTRS_NAME = \"lfw_attributes.txt\"\n","IMAGES_NAME = \"lfw-deepfunneled.tgz\"\n","RAW_IMAGES_NAME = \"lfw.tgz\"\n","\n","def decode_image_from_raw_bytes(raw_bytes):\n","    img = cv2.imdecode(np.asarray(bytearray(raw_bytes), dtype=np.uint8), 1)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    return img\n","\n","def load_lfw_dataset(\n","        use_raw=False,\n","        dx=80, dy=80,\n","        dimx=45, dimy=45):\n","\n","    # Read attrs\n","    df_attrs = pd.read_csv(ATTRS_NAME, sep='\\t', skiprows=1)\n","    df_attrs = pd.DataFrame(df_attrs.iloc[:, :-1].values, columns=df_attrs.columns[1:])\n","    imgs_with_attrs = set(map(tuple, df_attrs[[\"person\", \"imagenum\"]].values))\n","\n","    # Read photos\n","    all_photos = []\n","    photo_ids = []\n","\n","    # tqdm in used to show progress bar while reading the data in a notebook here, you can change\n","    # tqdm_notebook to use it outside a notebook\n","    with tarfile.open(RAW_IMAGES_NAME if use_raw else IMAGES_NAME) as f:\n","        for m in tqdm.tqdm_notebook(f.getmembers()):\n","            # Only process image files from the compressed data\n","            if m.isfile() and m.name.endswith(\".jpg\"):\n","                # Prepare image\n","                img = decode_image_from_raw_bytes(f.extractfile(m).read())\n","\n","                # Crop only faces and resize it\n","                img = img[dy:-dy, dx:-dx]\n","                img = cv2.resize(img, (dimx, dimy))\n","\n","                # Parse person and append it to the collected data\n","                fname = os.path.split(m.name)[-1]\n","                fname_splitted = fname[:-4].replace('_', ' ').split()\n","                person_id = ' '.join(fname_splitted[:-1])\n","                photo_number = int(fname_splitted[-1])\n","                if (person_id, photo_number) in imgs_with_attrs:\n","                    all_photos.append(img)\n","                    photo_ids.append({'person': person_id, 'imagenum': photo_number})\n","\n","    photo_ids = pd.DataFrame(photo_ids)\n","    all_photos = np.stack(all_photos).astype('uint8')\n","\n","    # Preserve photo_ids order!\n","    all_attrs = photo_ids.merge(df_attrs, on=('person', 'imagenum')).drop([\"person\", \"imagenum\"], axis=1)\n","\n","    return all_photos, all_attrs\n","\n","# Prépare le dataset et le charge dans la variable X\n","X, attr = load_lfw_dataset(use_raw=True, dimx=32, dimy=32)\n","# Normalise les images\n","X = X/255\n","# Sépare les images en données d'entraînement et de test\n","X_train, X_test = train_test_split(X, test_size=0.1, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n = 10\n","plt.figure(figsize=(20, 4))\n","for i in range(n):\n","\n","\n","    ax = plt.subplot(1, n, i+1)\n","    plt.imshow(X_train[i].reshape(32, 32, 3))\n","    plt.gray()\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","\n","plt.show()"],"metadata":{"id":"kt5l_VB7pI5i"},"execution_count":null,"outputs":[]}]}