{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPXtJqY+z7LgPkd0HacnA47"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Pour ce nouveau TP, nous allons manipuler les notions vues lors deux précédents TPs afin de mettre en place les mécanismes de *Retrieval Augmented Generation* (RAG). Ce mécanisme permet à un LLM de générer des réponses sourcées et d'améliorer ainsi la fiabilité du texte généré.\n","\n","<center> <img src=\"https://drive.google.com/thumbnail?id=1GwUiaFmVrKzfbFZywX8jd73zXSeZDhyy&sz=w1000\" style=\"width:800;height:400px;\"></center>\n","<caption><center><b> Illustration d'un pipeline de RAG </b></center></caption>\n","\n","\n","L'idée, représentée dans le diagramme ci-dessus, nécessite plusieurs étapes successives :\n","\n","-   On commence par indexer une base de connaissances (partie supérieure du graphe). Pour cela, on va découper cette base en unités de taille moyenne (typiquement, un ou plusieurs paragraphes) et utiliser un modèle de langage (de type BERT) pour associer à chaque unité de connaissance un descripteur, dont on espère qu'il porte une information sémantique représentative du contenu textuel. Dans l'espace des descripteurs, deux paragraphes de sens voisin devraient se retrouver à une distance assez faible l'un de l'autre.\n","\n","-   Lorsqu'un utilisateur tape ensuite un prompt pour un modèle génératif, on génère le descripteur de ce prompt et on le compare aux descripteurs de la base de connaissances, afin d'identifier les connaissances pertinentes pour répondre à la question posée.\n","\n","-   On ajoute ainsi le paragraphe correspondant en contexte du prompt afin d'aider le modèle de langage (de type GPT) à générer une réponse pertinente."],"metadata":{"id":"CaTZVY-QV0iv"}},{"cell_type":"markdown","source":["# Modèle de Langage \"instruction-tuned\"\n","\n","Les modèles de langage génératifs que nous avons manipulé au TP précédent sont très performants pour générer du texte mais nécessitent quelques modifications pour être utilisés au sein, par exemple, d'un client de chat (type ChatGPT). Les modèles \"instruction-tuned\" sont ainsi des modèles de langage génératifs, de type décodeur (i.e. de la famille de GPT) mais qui ont été *fine-tuned* avec un ensemble d'apprentissage spécifique leur permettant d'intégrer des informations supplémentaires lors de la génération.\n","\n","Ainsi, par exemple, il est possible de tagger le texte d'un prompt comme étant écrit soit par un utilisateur, soit par le modèle : ce mécanisme rend possible des \"conversations\" entre un utilisateur et le modèle, car il permet d'inclure dans le prompt courant les échanges précédents.\n","\n","```\n","<start_of_turn>user\n","knock knock<end_of_turn>\n","<start_of_turn>model\n","who is there<end_of_turn>\n","<start_of_turn>user\n","Gemma<end_of_turn>\n","<start_of_turn>model\n","Gemma who?<end_of_turn>\n","```\n","(exemple extrait de [la documentation de Gemma](https://ai.google.dev/gemma/docs/core/prompt-structure))\n","\n","Dans ce TP, nous allons utiliser le modèle de DataBricks dolly-v2-3B (pour *instruction-tuned*), qui est un modèle gratuit *instruction-tuned* relativement compact."],"metadata":{"id":"XTTn6cAkbpB_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7BtGVEcAVwYV"},"outputs":[],"source":["import torch\n","from transformers import pipeline\n","\n","generate_text = pipeline(model=\"databricks/dolly-v2-3b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\n"]},{"cell_type":"markdown","source":["Pour poser une question au modèle, vous pouvez utiliser les commandes suivantes :\n","\n","```\n","res = generate_text(\"Ask your question in english\")\n","print(res[0][\"generated_text\"])\n","```"],"metadata":{"id":"YLq1Wtt_e09F"}},{"cell_type":"markdown","source":["**Travail à faire** : faites des tests et essayez de trouver des questions qui mettent en erreur le modèle. N'hésitez pas à relancer la prédiction plusieurs fois pour évaluer la stabilité de la réponse du modèle."],"metadata":{"id":"hTtDCC9jfTx-"}},{"cell_type":"markdown","source":["Une fois que vous avez identifié une ou plusieurs questions pour lesquelles le modèle se trompe, cherchez (par exemple, sur Wikipedia) un paragraphe qui contient quelque part la réponse à ce ou ces questions. Il est possible de fournir un contexte à Dolly en structurant le prompt en différentes parties : instructions / Contexte / Réponse. Voici comment l'on peut s'y prendre :\n","\n","```\n","prompt = \"\"\"### Instruction:\n","Ask your question in english here\n","\n","### Context:\n","Copy/Paste here your paragraph of context\n","\n","### Response:\n","\"\"\"\n","\n","res = generate_text(prompt)\n","print(res[0][\"generated_text\"])\n","```"],"metadata":{"id":"Nq5oFcwRgMHO"}},{"cell_type":"markdown","source":["**Travail à faire** : Vérifiez que votre modèle utilise bien le contexte pour répondre maintenant correctement à votre question."],"metadata":{"id":"eyHceyemhIrx"}},{"cell_type":"markdown","source":["## Un exemple illustratif (mais c'est mieux si vous le faites vous-même)"],"metadata":{"id":"n3HGOq8QgAt_"}},{"cell_type":"markdown","source":["Demandons au modèle qui était le roi de France en 1700 ? Le modèle prédit correctement Louis XIV. En revanche, essayez un peu de modifier les dates. Louis XIV est mort en 1715, mais si vous demandez au modèle qui était le roi en 1716, il prédit également Louis XIV (bien que, de temps en temps, il prédise également Louis XV)."],"metadata":{"id":"LzW0ammFh0va"}},{"cell_type":"code","source":["res = generate_text(\"Who was the french king in 1716?\")\n","print(res[0][\"generated_text\"])"],"metadata":{"id":"yX9YrKPog1AT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nous allons maintenant donner du contexte au modèle pour faire sa prédiction. Copions ainsi le paragraphe suivant, extrait de [Wikipedia](https://en.wikipedia.org/wiki/Louis_XV) :    \n","\n","```\n","Louis XV (15 February 1710 – 10 May 1774), known as Louis the Beloved (French:\n","le Bien-Aimé),[1] was King of France from 1 September 1715 until his death in\n","1774. He succeeded his great-grandfather Louis XIV at the age of five. Until\n","he reached maturity (then defined as his 13th birthday) in 1723, the kingdom\n","was ruled by his grand-uncle Philippe II, Duke of Orléans, as Regent of\n","France. Cardinal Fleury was chief minister from 1726 until his death in\n","1743, at which time the king took sole control of the kingdom.\n","```\n","\n","\n","Pour fournir un contexte à Dolly, il faut structurer le prompt en différentes parties : instructions / Contexte / Réponse. Voici comment l'on peut s'y prendre :\n"],"metadata":{"id":"dmyG4POPkctJ"}},{"cell_type":"code","source":["prompt = \"\"\"### Instruction:\n","Who was the french king in 1716?\n","\n","### Context:\n","Louis XV (15 February 1710 – 10 May 1774), known as Louis the Beloved (French: le Bien-Aimé),[1] was King of France from 1 September 1715 until his death in 1774. He succeeded his great-grandfather Louis XIV at the age of five. Until he reached maturity (then defined as his 13th birthday) in 1723, the kingdom was ruled by his grand-uncle Philippe II, Duke of Orléans, as Regent of France. Cardinal Fleury was chief minister from 1726 until his death in 1743, at which time the king took sole control of the kingdom.### Response:\n","\"\"\"\n","\n","res = generate_text(prompt)\n","print(res[0][\"generated_text\"])"],"metadata":{"id":"fmEWzawCj2l8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Testez le code précédent plusieurs fois : vous verrez que les réponses du modèles oscillent maintenant entre Louis XV et Philippe II, Duc d'Orléans (réponse qui n'apparait jamais sans le contexte).\n","\n","En écrivant n'importe quoi dans le contexte du prompt, on peut ainsi faire dire n'importe quoi au modèle :"],"metadata":{"id":"CZkOVydSlM09"}},{"cell_type":"code","source":["prompt = \"\"\"### Instruction:\n","Who was the french king in 1716?\n","\n","### Context:\n","Axel Carlier (1700-1735) became King of France in 1715.\n","\"\"\"\n","\n","res = generate_text(prompt)\n","print(res[0][\"generated_text\"])"],"metadata":{"id":"6fGVzOB_ljQj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cet exemple illustre ainsi la nécessité de fournir une information fiable en contexte. C'est l'objet de la section suivante."],"metadata":{"id":"x8CnciG8lxsc"}},{"cell_type":"markdown","source":["# Indexation de texte aidée par un modèle de type BERT"],"metadata":{"id":"yH_ST5MzmInZ"}},{"cell_type":"markdown","source":["Dans cette partie nous allons voir comment indexer puis rechercher des paragraphes de texte à l'aide d'un modèle encodeur de type BERT.\n","\n","Nous allons illustrer cette partie à l'aide de 2 fichiers texte : l'un contient un texte de psychologie, l'autre d'écologie."],"metadata":{"id":"zAh1uE1-nzId"}},{"cell_type":"code","source":["!wget https://acarlier.fr/tp/ecology.txt\n","!wget https://acarlier.fr/tp/psychology.txt"],"metadata":{"id":"MWi_VRrlsUEh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Commençons par découper les textes en paragraphe (on décide ici que les paragraphes sont séparés par deux retours à la ligne). Chaque paragraphe est étiqueté en fonction du texte auquel il appartient (cela nous sera utile pour l'affichage)"],"metadata":{"id":"oeJCRqYrhzvP"}},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Charger les fichiers texte et extraire les paragraphes\n","def charger_paragraphes(chemin_fichier):\n","    with open(chemin_fichier, 'r', encoding='utf-8') as f:\n","        texte = f.read()\n","    # On découpe le texte par double saut de ligne (chaque paragraphe)\n","    paragraphes = [p.strip() for p in texte.split('\\n\\n') if p.strip()]\n","    return paragraphes\n","\n","# Charger les paragraphes des deux fichiers\n","paragraphes_fichier1 = charger_paragraphes('ecology.txt')\n","paragraphes_fichier2 = charger_paragraphes('psychology.txt')\n","\n","# Combiner tous les paragraphes dans une seule liste\n","tous_les_paragraphes = paragraphes_fichier1 + paragraphes_fichier2\n","\n","# Créer une liste d’étiquettes : 0 pour le fichier 1, 1 pour le fichier 2\n","etiquettes = [0] * len(paragraphes_fichier1) + [1] * len(paragraphes_fichier2)\n"],"metadata":{"id":"tTDFUspVmFq2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["On charge en suite un modèle encodeur nommé all-mpnet-base-v2 qui associe à chaque fragment de texte un descripteur de taille 768."],"metadata":{"id":"o0eaFxYAlmFZ"}},{"cell_type":"code","source":["# Génération des embeddings à l’aide du modèle all-mpnet-base-v2\n","modele = SentenceTransformer('all-mpnet-base-v2')\n","embeddings = modele.encode(tous_les_paragraphes)"],"metadata":{"id":"4BfgIpR1iMjv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Enfin, on utilise l'algorithme du t-SNE, très utilisé en visualisation, pour projeter les descripteurs dans un plan. On peut ainsi afficher les descripteurs, accompagnés d'une couleur correspondant au texte auquel appartient chaque paragraphe."],"metadata":{"id":"PMelkgPzl50L"}},{"cell_type":"code","source":["# Réduire les dimensions avec t-SNE (de 768 à 2)\n","tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n","embeddings_2D = tsne.fit_transform(embeddings)\n","\n","\n","couleurs = ['blue', 'green']  # Bleu pour le fichier 1, vert pour le fichier 2\n","plt.figure(figsize=(10, 8))\n","for i in range(len(embeddings_2D)):\n","    # On affiche un point coloré pour chaque paragraphe\n","    plt.scatter(\n","        embeddings_2D[i, 0], embeddings_2D[i, 1],\n","        color=couleurs[etiquettes[i]],\n","        label=f'Texte {etiquettes[i]+1}' if i == etiquettes.index(etiquettes[i]) else \"\"\n","    )\n","\n","# Ajouter la légende, le titre, les axes et une grille\n","plt.legend()\n","plt.title('t-SNE des embeddings de paragraphes')\n","plt.xlabel('Dimension 1')\n","plt.ylabel('Dimension 2')\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"ptVEb45ol65_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["On observe que les embeddings de paragraphes sont plus proches des autres embeddings du même texte que des embeddings de l'autre texte !\n","\n","Supposons maintenant qu'un utilisateur écrive un prompt. Nous pouvons comparer l'embedding du prompt à tous les embeddings de paragraphes afin de trouver les paragraphes les plus similaires. Une mesure de similarité classique pour cela est d'utiliser la similarité cosinus (correspondant au cosinus de l'angle formé entre les 2 descripteurs)."],"metadata":{"id":"Hv_HCvhlwX9L"}},{"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Fonction pour rechercher les paragraphes les plus similaires à une phrase\n","def trouver_paragraphes_similaires(phrase, model, paragraphes, paragraph_embeddings, top_k=3):\n","    # Générer l'embedding de la phrase\n","    embedding_phrase = model.encode([phrase])\n","\n","    # Calculer la similarité cosinus avec tous les paragraphes\n","    similarites = cosine_similarity(embedding_phrase, paragraph_embeddings)[0]\n","\n","    # Récupérer les indices des top_k paragraphes les plus similaires\n","    indices_top = np.argsort(similarites)[-top_k:][::-1]  # Tri décroissant\n","\n","    # Afficher les résultats\n","    print(f\"\\nPhrase de requête : \\\"{phrase}\\\"\\n\")\n","    print(\"Paragraphes les plus similaires :\\n\")\n","    for i, idx in enumerate(indices_top):\n","        print(f\"--- Résultat {i+1} (similarité : {similarites[idx]:.4f}) ---\")\n","        print(paragraphes[idx])\n","        print()\n","\n","# Exemple d'utilisation\n","phrase_recherche = \"How can we monitor species population in real time.\"\n","trouver_paragraphes_similaires(\n","    phrase=phrase_recherche,\n","    model=modele,\n","    paragraphes=tous_les_paragraphes,\n","    paragraph_embeddings=embeddings,\n","    top_k=3\n",")"],"metadata":{"id":"EO3WyFtdtDsj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["phrase_recherche = \"Is it possible to learn how to better control our emotions?\"\n","trouver_paragraphes_similaires(\n","    phrase=phrase_recherche,\n","    model=modele,\n","    paragraphes=tous_les_paragraphes,\n","    paragraph_embeddings=embeddings,\n","    top_k=3\n",")"],"metadata":{"id":"fY0hf_OBtj7h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["On voit bien que les paragraphes les plus similaires au prompt traitent à peu près du même sujet et permettent de donner des éléments de réponse à la question posée. En combinant ce mécanisme à celui de la section précédente, on peut ainsi mettre en place le RAG."],"metadata":{"id":"mAGhkri4m1VS"}},{"cell_type":"markdown","source":["# Partie libre : à vous de jouer"],"metadata":{"id":"dsgxzFirnJGe"}},{"cell_type":"markdown","source":["Vous avez maintenant tous les éléments pour mettre en place vous-même un pipeline de RAG.\n","\n","**Travail à faire**\n","-   trouver un sujet pour lequel le modèle Dolly répond mal aux questions qu'on lui pose\n","-   trouver un document à indexer qui traite de ce sujet (par exemple, une ou plusieurs pages Wikipedia, des cours, etc.) et utiliser all-mpnet-base-v2 pour indexer les informations de ce document\n","-   Intégrer dans un prompt les questions et les informations automatiquement récupérées pour voir si le modèle parvient à répondre plus efficacement aux questions que vous avez identifiées."],"metadata":{"id":"k5qGi2X3nN0P"}},{"cell_type":"code","source":[],"metadata":{"id":"AFgvaGS-nM5n"},"execution_count":null,"outputs":[]}]}