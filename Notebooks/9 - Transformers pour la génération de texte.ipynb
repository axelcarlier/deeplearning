{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Pour ce nouveau TP, nous allons poursuivre  l'utilisation des Transformers pour le traitement du langage naturel à travers la tâche devenue emblématique du domaine : la **génération de texte**.\n","\n","Pour cela nous allons utiliser une base de données de tweets écrits par Donald Trump sur son compte twitter (@realdonaldtrump) entre 2009 et 2020.\n","\n","<center><img src=\"https://www.alternatives-economiques.fr/sites/default/files/public/styles/16x9/public/field/image/000_36tp6xn.webp?orig=jpg&itok=bKKkrbct\" width=600> </center>\n","\n","A l'issue de l'entraînement, on obtiendra un modèle permettant de générer, par exemple, les phrases suivantes :\n","\n","```I love Putin. He is a great guy. But I think he is a weak man. I see him as a very tough guy. He is not very smart, but he is very strong. He has a lot of things to say.``` (obtenu à partir d'une phrase de départ 'I love Putin')\n","\n","```Obama is  the most corrupt, the most corrupt President ever. As a private citizen I have no interest in paying him any money. He is a fraud!``` (obtenu à partir de la phrase de départ 'Obama is')\n","\n","**NB:** Vous allez voir que la quantité de code à compléter dans ce TP est relativement faible. Si vous terminez en avance, n'hésitez pas à réfléchir à comment adapter ce code à votre projet !\n","\n","\n"],"metadata":{"id":"DBlkTreEQS04"}},{"cell_type":"markdown","source":["# Présentation des données"],"metadata":{"id":"p7k2_Q2Bcevc"}},{"cell_type":"markdown","source":["Commencez par télécharger les données :"],"metadata":{"id":"-Tmi5HK0QjOs"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from keras import layers, Model\n","from keras import models\n","import csv\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n","# Téléchargement des données\n","path_to_file = tf.keras.utils.get_file('realdonaltrump.csv', 'https://drive.google.com/uc?export=download&id=1s1isv9TQjGiEr2gG__8bOdBFvQlmepRt')\n","\n","# Lecture du CSV dans une liste de tweets et concaténation des tweets dans la variable text\n","tweets = []\n","text = ''\n","with open(path_to_file, newline='') as csvfile:\n","    reader = csv.DictReader(csvfile)\n","    for row in reader:\n","        tweets.append(row['content'])\n","        text += row['content']\n","\n","\n","# Affichage des 10 premiers tweets\n","print(tweets[:10])"],"metadata":{"id":"36H07MywchIW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Le fichier CSV compte 43339 tweets, accompagnés d'autres métadonnées que nous n'utiliserons pas.\n","\n"],"metadata":{"id":"KVMw_6NEQmfq"}},{"cell_type":"code","source":["# Nombre total de caractères du dataset\n","print(f'Longueur totale du texte: {len(text)} caractères')"],"metadata":{"id":"foL5SND0sMQG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extraction des caractères uniques du texte\n","vocab = sorted(set(text))\n","print(f'{len(vocab)} unique caractères')"],"metadata":{"id":"LVgg7F6gsg-w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calcul des fréquences d'apparition de chaque caractère\n","char_freq = {}\n","for char in text:\n","  if char not in char_freq:\n","    char_freq[char] = 1\n","  else:\n","    char_freq[char] += 1\n","\n","\n","# Affichage du vocabulaire trié par ordre de fréquence décroissant en filtrant les caractères apparaissant moins de 100 fois\n","for char in sorted(char_freq, key=lambda x: char_freq[x], reverse=True):\n","  print(f'{char}: {char_freq[char]}')\n","\n"],"metadata":{"id":"IKo3YevosjVC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nous avons donc 147 caractères dans la base de données, dont certains sont extrêmement rares. Il aurait certainement été possible de remplacer les caractères rares par un token particulier (\\<unk\\>, pour *unknown*) mais ils sont suffisamment rares pour ne pas causer de problèmes particuliers."],"metadata":{"id":"PkPqVY9p9HhR"}},{"cell_type":"markdown","source":["# Entraînement complet d'un modèle relativement petit\n","\n","## Tokenization et formatage des données\n","\n","Pour plus de simplicité dans cette partie, nous allons adopter une *Tokenization* basée sur les caractères. Chaque caractère se verra assigner un identifiant unique, et nous allons ajouter deux tokens supplémentaires, l'un pour désigner le début d'un tweet (\\<sot\\>, *start of tweet*), l'autre pour en désigner la fin (\\<eot\\>).\n","\n","Nous allons créer deux structures de données permettant de passer d'une représentation caractère vers les indices des tokens (```char_to_ind```) et vice versa (```ind_to_char```)."],"metadata":{"id":"GUfnsU85t10M"}},{"cell_type":"code","source":["# Création du dictionnaire chatr_to_ind pour associer un indice unique à chaque caractère, en commençant à l'indice 2\n","char_to_ind = {char: idx+2 for idx, char in enumerate(vocab)}\n","char_to_ind['<sot>'] = 0 # Start of tweet\n","char_to_ind['<eot>'] = 1 # End of tweet\n","\n","# Création du dictionnaire ind_to_char pour associer à chaque indice un caractère\n","ind_to_char = {idx: char for char, idx in char_to_ind.items()}\n","\n","\n","print(char_to_ind)\n","print(ind_to_char)"],"metadata":{"id":"g31ZFDnV9BMA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nous pouvons maintenant passer à la Tokenization de la base de données, c'est-à-dire à la conversion de chaque tweet en une liste d'identifiants représentant le tweet. Par exemple, pour le premier tweet :"],"metadata":{"id":"iio2rA4I9Agp"}},{"cell_type":"code","source":["print(tweets[0])\n","\n","tokenized_tweet_0 = [char_to_ind['<sot>']]\n","for char in tweets[0]:\n","  tokenized_tweet_0.append(char_to_ind[char])\n","tokenized_tweet_0.append(char_to_ind['<eot>'])\n","\n","print(tokenized_tweet_0)\n"],"metadata":{"id":"avKG5oZg-ni0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notez que nous avons transcrit le tweet en rajoutant \\<sot\\> au début de la liste et \\<eot\\> à la fin."],"metadata":{"id":"peuGYPjw-0sP"}},{"cell_type":"code","source":["# Parcours de la liste de tweets et création d'une liste de listes d'indices des caractères composant chaque tweet\n","tweets_ind = []\n","for tweet in tqdm(tweets):\n","  tweet_ind = [char_to_ind['<sot>']]\n","  for char in tweet:\n","    tweet_ind.append(char_to_ind[char])\n","  tweet_ind.append(char_to_ind['<eot>'])\n","  tweets_ind.append(tweet_ind)\n"],"metadata":{"id":"wpf_s6rRt4H6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Il est intéressant de visualiser l'ordre de grandeur des tweets une fois tokenizés :"],"metadata":{"id":"eQs48m2B_B4P"}},{"cell_type":"code","source":["# Affichage d'un histogramme des longueurs des tweets tokenizés\n","tweet_lengths = [len(tweet) for tweet in tweets_ind]\n","plt.hist(tweet_lengths, bins=100)\n","plt.title('Histogramme des longueurs des tweets')\n","plt.xlabel('Longueur')\n","plt.show()"],"metadata":{"id":"p0TbX3S2rZFv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["La grande majorité des tweets ont moins de 150 caractères. Ceci permettra de choisir la taille du contexte un peu plus tard.\n","\n","\n","Nous allons maintenant formater les données pour l'apprentissage. Notre modèle de langage sera un modèle *Many-to-many* qui prédira pour chaque token de la séquence le token suivant. Souvenez-vous, nous avons fait la même chose l'an passé en TP sur les RNN (à l'époque, avec des noms de famille).\n","\n","<center><img src=\"https://drive.google.com/uc?id=1TfgprY0yB4blIlRbHHr3S8UQB3nid5zo\" width=600> </center>\n","<caption><center> Génération de nom à l'aide d'une cellule récurrente</center></caption>\n","\n","Nous devons donc créer des matrices X et Y contenant des séquences de tweets, où les séquences de Y sont décalées d'un cran vers la droite.\n","\n","Pour cela, nous allons commencer par concaténer l'intégralité des tweets tokenizés du dataset :"],"metadata":{"id":"B5WJiTlM_Kz1"}},{"cell_type":"code","source":["# Concaténation de tous les tweets tokenizés dans une seule liste\n","all_tweets_ind = []\n","for tweet in tweets_ind:\n","  all_tweets_ind += tweet\n","\n","print(len(all_tweets_ind))"],"metadata":{"id":"GevsK_karuVe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nous allons maintenant fixer une taille de contexte. Ceci est un hyperparamètre que vous pourrez ajuster un peu plus tard, mais **gardez à l'esprit que les couches d'attention ont une complexité qui croit avec le carré de la taille du contexte**. Autrement dit, plus le contexte est petit, plus votre modèle sera rapide. Mais, bien sûr, moins il conservera de mémoire pour générer la fin d'un tweet à partir du début."],"metadata":{"id":"1psZElFPAKUv"}},{"cell_type":"code","source":["MAX_LEN = 50"],"metadata":{"id":"ywi7pAc4AjdN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pour créer les variables X et Y, nous allons découper l'intégralité de notre dataset en morceaux de longueur ```MAX_LEN```+1. Chaque morceau sera ensuite réparti entre la variable X (les ```MAX_LEN``` premiers tokens) et la variable Y (les ```MAX_LEN``` derniers tokens)."],"metadata":{"id":"gKvCNhQBAiyU"}},{"cell_type":"code","source":["import numpy as np\n","import math\n","\n","# Création des variables X et Y\n","# On découpe l'ensemble des tweets concaténés en MAXLEN+1 caractères consécutifs : les MAXLEN premiers sont les x et les MAXLEN derniers sont les y\n","### A COMPLETER\n","\n","\n","print(X, Y)"],"metadata":{"id":"ImHrV4gmr2wT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Résultat attendu** :\n","```\n","[[ 0. 36. 70. ... 66. 85. 70.]\n"," [48. 74. 72. ... 80. 81.  2.]\n"," [70. 79.  2. ... 72.  2. 80.]\n"," ...\n"," [70.  2. 85. ... 90.  2. 85.]\n"," [79. 74. 72. ... 84. 81. 67.]\n"," [26. 58.  1. ... 17. 56. 46.]] [[36. 70.  2. ... 85. 70.  2.]\n"," [74. 72. 73. ... 81.  2. 54.]\n"," [79.  2. 46. ...  2. 80. 79.]\n"," ...\n"," [ 2. 85. 80. ...  2. 85. 80.]\n"," [74. 72. 73. ... 81. 67. 55.]\n"," [58.  1.  0. ... 56. 46. 77.]]\n"," ```"],"metadata":{"id":"PrB3NWbMBD5q"}},{"cell_type":"markdown","source":["**NB** : peut-être êtes vous choqué·e par le fait que certaines données de notre ensemble d'apprentissage se retrouvent ainsi \"à cheval\" sur deux tweets consécutifs, qui n'ont potentiellement rien à voir.\n","\n","Cela nous permet d'éviter l'écueil de devoir gérer les différences de longueur des tweets. Si nous avions choisi une taille de contexte de, disons, 300 (autour de la longueur maximum des tweets), nous aurions dû compléter la grande majorité des tweets avec un token spécial (pour faire du *padding*, \\<pad\\>). Ce token aurait alors été en écrasante majorité dans la base de données, et le modèle aurait eu plus de mal à apprendre à cause du déséquilibre de classes induit par cette solution."],"metadata":{"id":"bjMWeLgjBMNj"}},{"cell_type":"markdown","source":["## Création du modèle\n","\n","Pour cette étape, je vous renvoie aux explications du TP4 de la semaine passée car vous allez pouvoir réutiliser le code écrit à cette occasion.\n","\n","Du point de vue de l'implémentation, une unique chose sépare un réseau de type **GPT** (modèle décodeur, que nous implémentons cette semaine) d'un réseau de type **BERT** (modèle encodeur, implémenté au TP4) : la couche d'auto-attention.\n","\n","Dans **BERT**, qui est **B**idirectionnel, l'attention d'un token peut être portée à tous les tokens de la séquence, qu'ils soient situés avant ou après dans l'ordre de la séquence.\n","\n","Dans **GPT**, qui sera utilisé pour faire de la **G**énération, il est strictement interdit de porter attention à des tokens qui sont postérieurs au token courant ! En effet, comme l'objectif de **GPT** est de générer progressivement du texte, cela n'aurait aucun sens de porter attention à des tokens que nous allons générer dans le futur.\n","\n","A vous de lire attentivement la documentation de la classe [MultiHeadAttention](https://keras.io/api/layers/attention_layers/multi_head_attention/) pour déterminer quel paramètre l'on doit positionner poru masquer la couche d'attention."],"metadata":{"id":"E92yOKidtKjQ"}},{"cell_type":"code","source":["class TransformerBlock(layers.Layer):\n","    # embed_dim désigne la dimension des embeddings maintenus à travers les différentes couches,\n","    # et num_heads le nombre de têtes de la couche d'attention.\n","    # DANS CETTE FONCTION, ON NE FAIT QUE DEFINIR LES COUCHES\n","    def __init__(self, embed_dim, num_heads):\n","        super().__init__()\n","        # Définition des différentes couches qui composent le bloc\n","        # Couche d'attention\n","        self.att =\n","        # Première couche de Layer Normalization\n","        self.layernorm1 =\n","        # Couche Dense (Feed-Forward)\n","        self.ffn =\n","        # Deuxième couche de normalisation\n","        self.layernorm2 =\n","\n","    # DANS CETTE FONCTION, ON APPELLE EXPLICITEMENT LES COUCHES DEFINIES DANS __init__\n","    # ON PROPAGE DONC LES ENTREES inputs A TRAVERS LES DIFFERENTES COUCHES POUR OBTENIR\n","    # LA SORTIE\n","    def call(self, inputs):\n","        # Application des couches successives aux entrées\n","        return ..."],"metadata":{"id":"E8IhBm6QtMu3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["La partie Embedding est inchangée :"],"metadata":{"id":"J5fmllcPDXv_"}},{"cell_type":"code","source":["class TokenAndPositionEmbedding(layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim):\n","        super().__init__()\n","        # Définition des différentes couches qui composent le bloc Embedding\n","        # Embedding de mot\n","        self.token_emb =\n","        # Embedding de position\n","        self.pos_emb =\n","\n","    def call(self, x):\n","        # Calcul de l'embedding à partir de l'entrée x\n","        # ATTENTION : UTILISER UNIQUEMENT DES FONCTIONS TF POUR CETTE PARTIE\n","        # Récupération de la longueur de la séquence\n","        maxlen =\n","        # Création d'un vecteur [0, 1, ..., maxlen] des positions associées aux\n","        # mots de la séquence (fonction tf.range)\n","        positions =\n","        # Calcul des embeddings de position\n","        positions_emb = ...\n","        # Calcul des embeddings de mot\n","        words_emb = ...\n","        return"],"metadata":{"id":"F_4I_WsDtgmY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Enfin, vous devez assembler un réseau qui ressemble à GPT (ci-dessous) :    \n","\n","\n","<center><img src=\"https://drive.google.com/uc?id=1w1CyLROPq-EWMd-Spr6wR596QEx1KpNa\" width=200> </center>\n","<caption><center>  Schéma de l'architecture de GPT</center></caption>\n","\n","Je vous propose des valeurs pour EMBED_DIM et NUM_HEADS, mais vous êtes bien sûr libres de les modifier.\n","\n","Pour obtenir des résultats similaires aux miens, vous devez utiliser 3 blocs transformers, et terminer le réseaux avec une couche dense de 100 neurones, et enfin la couche de sortie (à vous de déterminer quoi y mettre !)\n"],"metadata":{"id":"QRw39S4IDbj8"}},{"cell_type":"code","source":["EMBED_DIM = 128  # Dimension de l'embedding pour chaque mot\n","NUM_HEADS = 8  # Nombre de têtes d'attention\n","VOCAB_SIZE = len(char_to_ind)\n","### A COMPLETER\n","inputs = keras.Input(shape=(MAX_LEN,))\n","embedding_layer = ...\n","...\n","outputs = keras.layers.Dense(...\n","\n","model = keras.Model(inputs=inputs, outputs=outputs)\n","model.summary()"],"metadata":{"id":"3oxBesfJtmFg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Il reste à entraîner le modèle. Pour faire en sorte que ça ne soit pas trop long, je n'ai mis que 5 epochs, mais des entrainements plus longs conduisent à de meilleurs résultats.\n","\n","Grosse difficulté de cette partie : quelle fonction de coût devez-vous utiliser ? Un petit indice, vous la trouverez sur [cette page](https://keras.io/api/losses/) au rayon *Probabilistic losses*."],"metadata":{"id":"-Q2hi_SmEbrI"}},{"cell_type":"code","source":["### A COMPLETER\n","model.compile(\n","    optimizer=\"adam\", loss=\"...\", metrics=[\"accuracy\"]\n",")\n","history = model.fit(\n","    X, Y, batch_size=16, epochs=5)"],"metadata":{"id":"EEDWymGNuGOv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Génération d'un résultat"],"metadata":{"id":"NqjlnZgmu9xA"}},{"cell_type":"markdown","source":["Profitez du temps d'entrainement de la cellule précédente pour prendre un café et lire le code permettant d'implémenter la génération. Il n'y a rien à compléter mais ce code n'est pas si facile à comprendre."],"metadata":{"id":"Y8ksStATE-Sz"}},{"cell_type":"code","source":["# Début de séquence : mettre '' si l'on veut générer un nom de zéro\n","seed_seq = 'Obama is'\n","\n","i = 0\n","\n","# Création de la séquence qui va être fournie en entrée du réseau :\n","# On ajoute un token <sos> au démarrage, et on transcrit en la séquence d'id correspondante\n","input_seq = [char_to_ind['<sot>']]\n","for s in seed_seq:\n","  input_seq.append(char_to_ind[s])\n","  i+=1\n","\n","last_char = -1\n","\n","\n","# On génère des séquences de taille inférieure à MAX_LEN, et on s'arrête lorsque\n","# l'on génère un token <eot> (id 1)\n","while last_char != 1 and i < 200:\n","  # La séquence d'entrée doit être de dimension BATCH_SIZE x MAX_LEN\n","  # soit en fait ici 1 x MAX_LEN\n","  input = np.zeros((MAX_LEN))\n","  if i < MAX_LEN:\n","    input[:i+1] = np.array(input_seq)\n","  else:\n","    input = np.array(input_seq[-MAX_LEN:])\n","\n","  input = np.expand_dims(input, 0)\n","\n","  # Prédiction du modèle sur la séquence en cours\n","  pred = model.predict(input, verbose=0)\n","  # En sortie, on a 1 x MAX_LEN x 149\n","\n","  # Échantillonnage du caractère généré à partir de la distribution de probabilité\n","  # prédite par le modèle pour le dernier élément de la séquence\n","  last_char = np.random.choice(VOCAB_SIZE, 1, p=pred[0, min(i, MAX_LEN-1)])[0]\n","\n","  # Ajout du caractère à la séquence générée\n","  input_seq.append(last_char)\n","  i += 1\n","\n","# Affichage du nom généré\n","generated_tweet = ''\n","for s in input_seq:\n","  if s != 0 and s != 1:\n","    generated_tweet+=ind_to_char[s]\n","\n","print(generated_tweet)"],"metadata":{"id":"fvFrhasCvAd3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Voici quelques exemples de résultats que j'obtiens après 5 epochs (la loss est autour de 1,5, les résultats s'améliorent un peu à mesure qu'elle baisse) :     \n","\n","```Obama is no manufacture, there's pruingistricty. Think IME DENDY DUNT NATED more to construction, the holden for working on foom reproud & thank you!pic.twitter.com/wNXxY03ETLbE```\n","\n","```Obama is a lot of @ CNN and the This is country.```\n","\n","```Obama is an oposet bad all times and hill out the world in me.We dected were progress on @ dmiasonstroning Bernie, VOTES and ar the crime night Donumer more,. Laput are . I create pla att to bluin Sco```\n","\n","Les résultats peuvent paraître décevants, mais la plupart des mots générés existent : c'est déjà satisfaisant, car le modèle les génère caractère par caractère !\n","\n","En augmentant la profondeur du réseau (nombre de blocs transformers), la dimension des *Embeddings* et bien sûr le nombre d'epochs, il est possible de faire mieux mais nous serons de toute façon limités par la Tokenization caractère qui rend le problème un peu plus difficile.\n","\n","Il est plus intéressant d'explorer plutôt le *fine-tuning* d'un modèle de langage déjà pré-entraîné.\n","\n"],"metadata":{"id":"JQvY_KbrFQ4s"}},{"cell_type":"markdown","source":["# Fine-tuning d'un petit LLM"],"metadata":{"id":"IuO5og2a8eU9"}},{"cell_type":"markdown","source":["Commencez par installer toutes les librairies suivantes. Cela prendra un peu de temps (2e café), et vous risquez de devoir redémarrer votre machine virtuelle."],"metadata":{"id":"CmqOApuNGqX_"}},{"cell_type":"code","source":["!pip install transformers\n","!pip install datasets\n","!pip install peft\n","!pip install accelerate\n","!pip install bitsandbytes"],"metadata":{"id":"wVzxY80C8lms"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nous allons utiliser le modèle [OPT](https://arxiv.org/abs/2205.01068), pour *Open Pretrained Transformer* sorti par Meta en 2022. Il s'agit d'une collection de LLM de tailles allant de 125 millions de paramètres à 175 milliards de paramètres, pré-entraînés, et complètement libres d'utilisation.\n","\n","Comme nous l'avons vu la semaine passée, chargeons le Tokenizer et le modèle associés :"],"metadata":{"id":"3384gGKyG11u"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","model_name = \"facebook/opt-125m\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","model = AutoModelForCausalLM.from_pretrained(model_name, force_download=True)"],"metadata":{"id":"1EWWAOEK85jH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dans le bloc suivant, nous préparons la base de données en créant un objet Dataset à partir de nos tweets (non tokenizés), et en les passant ensuite dans le Tokenizer de opt-125m."],"metadata":{"id":"ycD0Jt_LHfNB"}},{"cell_type":"code","source":["from datasets import Dataset\n","\n","dataset = Dataset.from_dict({\"text\": tweets})\n","print(dataset)\n","\n","def tokenize_function(examples):\n","    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n","    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n","    return tokenized_inputs\n","\n","tokenized_dataset = dataset.map(tokenize_function, batched=True)"],"metadata":{"id":"xf81Ptno-g1a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pour accélérer l'entraînement, nous utilisons une méthode que je n'ai pas eu le temps de vous présenter en cours :    [LoRA](https://arxiv.org/abs/2106.09685), pour *Low Rank Approximation*.\n","\n","L'idée de cette méthode est de modifier incrémentalement les poids synaptiques d'un réseau à l'aide d'une décomposition matricielle.\n","\n","Ainsi une matrice $W \\in \\mathbb{R}^{d \\times k}$ représentant les poids synaptiques d'une couche du réseau peut s'écrire\n","$$ W' = W + AB$$\n","où $A \\in \\mathbb{R}^{d \\times r}$ et $B \\in \\mathbb{R}^{r \\times k}$, avec $r$ un rang très petit par rapport aux dimensions $d$ et $k$.\n","\n","Les deux matrices $A$ et $B$ n'ont que peu de paramètres, et leur produit $AB$ sera de rang très faible. La méthode LoRA, utilisée pour le fine-tuning de LLM, consiste à bloquer les paramètres de $W$ et à autoriser uniquement l'entraînement de $A$ et $B$. Malgré leur faible nombre de paramètres, le résultat permet de modifier significativement les prédictions du LLM tout en étant plus léger à entraîner, et moins sujet au sur-apprentissage."],"metadata":{"id":"EDP5bCbEHzoB"}},{"cell_type":"code","source":["from peft import LoraConfig, get_peft_model\n","\n","lora_config = LoraConfig(\n","    r=8,\n","    lora_alpha=16,\n","    target_modules=[\"q_proj\", \"v_proj\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","model = get_peft_model(model, lora_config)\n","model.print_trainable_parameters()"],"metadata":{"id":"ihJ3xpgT-zcU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nous pouvons maintenant lancer le *fine-tuning*"],"metadata":{"id":"LtjdoNLxKBRR"}},{"cell_type":"code","source":["from transformers import TrainingArguments, Trainer\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    max_steps=1000,\n","    logging_dir='./logs',\n","    logging_steps=10,\n","    eval_strategy=\"no\",\n","    save_strategy=\"epoch\",\n","    report_to=\"none\"\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset,\n","    tokenizer=tokenizer,\n",")\n","\n","trainer.train()"],"metadata":{"id":"Bb74wu8H-4hM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Le bloc suivant permet de définir le processus de génération de nouveaux tweets à partir de notre modèle. Il est possible d'initialiser ces tweets avec un prompt :"],"metadata":{"id":"QCi-koaCKFKe"}},{"cell_type":"code","source":["from transformers import pipeline\n","\n","# Configurer un pipeline de génération\n","generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n","\n","prompt = \"Obama is \"\n","results = generator(prompt, max_length=50, num_return_sequences=3, do_sample=True, temperature=0.7)\n","\n","for i, result in enumerate(results):\n","    print(f\"--- Génération {i+1} ---\")\n","    print(result['generated_text'])"],"metadata":{"id":"13iVN9ZuJC1d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Si le code précédent est trop long à exécuter, vous pouvez charger les poids suivants, que j'ai moi-même fine-tunés pendant 3 epochs (~45 min sur Google Colab)"],"metadata":{"id":"0_RfIH9xKxWS"}},{"cell_type":"code","source":["!wget https://acarlier.fr/tp/fine_tuned_opt_125m.zip\n","!unzip fine_tuned_opt_125m.zip\n","\n","# Path to the extracted model\n","model_path = './'\n","\n","# Load the model and tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","model = AutoModelForCausalLM.from_pretrained(model_path)"],"metadata":{"collapsed":true,"id":"u-HgjS3lKwiq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import pipeline\n","\n","# Configurer un pipeline de génération\n","generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n","\n","prompt = \"Obama is \"\n","results = generator(prompt, max_length=50, num_return_sequences=3, do_sample=True, temperature=0.7)\n","\n","for i, result in enumerate(results):\n","    print(f\"--- Génération {i+1} ---\")\n","    print(result['generated_text'])"],"metadata":{"id":"LI-Vti6gNGad"},"execution_count":null,"outputs":[]}]}