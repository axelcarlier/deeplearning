{"cells":[{"cell_type":"markdown","source":["# Introduction à la librairie Keras"],"metadata":{"id":"1qjWcjWFVcs7"}},{"cell_type":"markdown","source":["Dans le TP précédent, vous avez implémenté l'apprentissage et l'inférence d'un réseau de neurones. En pratique, il est plus courant de faire appel à des librairies qui masquent la complexité de ces algorithmes (notamment le calcul des gradients, réalisé par différentiation automatique). Dans la suite, nous utiliserons pour les TPs la librairie ***Keras***. Dans un premier temps, pour ce TP nous allons détailler sur un exemple simple (le même que pour le TP précédent) les différentes étapes à mettre en place pour entraîner un réseau à l'aide de cette librairie."],"metadata":{"id":"77ojmk9zVgUt"}},{"cell_type":"markdown","source":["Le bloc de code ci-dessous sera utile pour analyser l'évolution des performances au cours de l'apprentissage"],"metadata":{"id":"XRcZuPZm94b6"}},{"cell_type":"code","source":["def plot_training_analysis(history):\n","  acc = history.history['accuracy']\n","  val_acc = history.history['val_accuracy']\n","  loss = history.history['loss']\n","  val_loss = history.history['val_loss']\n","\n","  epochs = range(len(acc))\n","\n","  #plt.figure()\n","  plt.subplots(1, 2, figsize=(10, 4))\n","\n","\n","  plt.subplot(1,2,1)\n","  plt.plot(epochs, acc, 'b', linestyle=\"--\",label='Training acc')\n","  plt.plot(epochs, val_acc, 'g', label='Validation acc')\n","  plt.title('Training and validation accuracy')\n","  plt.legend()\n","\n","  plt.subplot(1,2,2)\n","  plt.plot(epochs, loss, 'b', linestyle=\"--\",label='Training loss')\n","  plt.plot(epochs, val_loss,'g', label='Validation loss')\n","  plt.title('Training and validation loss')\n","  plt.legend()\n","\n","  plt.show()\n","\n"],"metadata":{"id":"yCHH7owU93pQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exemple de classification binaire simple\n","\n"],"metadata":{"id":"b2Sq7AygNuNL"}},{"cell_type":"markdown","source":["Chargement des données (ce sont les mêmes que lors du dernier TP)"],"metadata":{"id":"fzh7_5kovuVl"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn import datasets\n","import matplotlib.pyplot as plt\n","\n","# Génération des données\n","x, y = datasets.make_blobs(n_samples=250, n_features=2, centers=2, center_box=(- 3, 3), random_state=1)\n","# Partitionnement des données en apprentissage et test\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=1)\n","\n","# Affichage des données d'apprentissage\n","plt.plot(x_train[y_train==0,0], x_train[y_train==0,1], 'b.', label='Apprentissage, classe 0')\n","plt.plot(x_train[y_train==1,0], x_train[y_train==1,1], 'r.', label='Apprentissage, classe 1')\n","\n","plt.xlabel('x1')\n","plt.ylabel('x2')\n","\n","# Affichage des données de test\n","plt.plot(x_test[y_test==0,0], x_test[y_test==0,1], 'b+', label='Test, classe 0')\n","plt.plot(x_test[y_test==1,0], x_test[y_test==1,1], 'r+', label='Test, classe 1')\n","\n","plt.legend()\n","plt.show()"],"metadata":{"id":"luY3XIU7WfWQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dans ce problème, les données $x^{(i)}$ sont les points, on a donc $x^{(i)} \\in \\mathbb{R}^2$. Les étiquettes $y^{(i)}$ correspondent à la couleur des points (bleue ou rouge), matérialisée par un label (0 ou 1).\n","\n","Pour résoudre le problème, on peut donc construire un perceptron monocouche à 2 entrées et une sortie.\n","\n","<center>\n","<img src=\"https://drive.google.com/thumbnail?id=1WfUL0fTE7fjeHMrlGjgH3b_3511ttnO8&sz=w1000\">\n","</center>\n","\n","Il s'agit d'un problème de classification binaire, il faut donc utiliser une fonction d'activation **sigmoide** sur la couche de sortie."],"metadata":{"id":"CCWqmKF2wOCX"}},{"cell_type":"code","source":["import tensorflow\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","# Définition du modèle, un simple perceptron monocouche.\n","model = Sequential() # Dans un modèle séquentiel, on peut ajouter les couches les unes après les autres\n","# Ici, on définit une seule couche connectant 2 neurones d'entrée à un neurone de sortie, portant une activation sigmoide\n","model.add(Dense(1, activation='sigmoid', input_dim=2)) # input_dim indique la dimension de la couche d'entrée, ici 2\n","\n","model.summary() # affiche un résumé du modèle"],"metadata":{"id":"lTGP4a9WXWpU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Le résumé du modèle (*summary*) nous indique le nombre de paramètres du modèle, ici 3 : 2 poids synaptiques et un biais.\n","\n","Il nous faut maintenant choisir la fonction de coût : pour ce problème de classification binaire, le choix standard est l'entropie croisée binaire (*binary cross-entropy*).\n","\n","On choisit dans un premier temps un optimiseur standard, la descente de gradient stochastique.\n"],"metadata":{"id":"bqcO-PZMxbpx"}},{"cell_type":"code","source":["from tensorflow.keras import optimizers\n","\n","# Définition de l'optimiseur\n","sgd = optimizers.SGD(learning_rate=0.1) # On choisit la descente de gradient stochastique, avec un taux d'apprentissage de 0.1\n","\n","# On définit ici, pour le modèle introduit plus tôt, l'optimiseur choisi (la SGD), la fonction de perte (ici\n","# l'entropie croisée binaire pour un problème de classification binaire) et les métriques que l'on veut observer pendant\n","# l'entraînement. L'accuracy désigne le pourcentage de bonnes classification, et est un indicateur plus simple à interpréter que l'entropie croisée.\n","model.compile(optimizer=sgd,\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Entraînement du modèle avec des mini-batchs de taille 20, sur 15 epochs.\n","# Le paramètre validation_split signifie qu'on tire aléatoirement une partie des données\n","# (ici 20%) pour servir d'ensemble de validation\n","history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=15, batch_size=20)\n","\n","plot_training_analysis(history)"],"metadata":{"id":"nZEDm5I-Lu-p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["L'apprentissage semble bien se passer : la perte d'apprentissage et la perte de validation décroissent au cours du temps, et l'accuracy est proche de 100% à la fin de l'apprentissage.\n","\n","La cellule suivante introduit une fonction permettant de visualiser la frontière de décision du modèle appris."],"metadata":{"id":"fD8fXJj0WJID"}},{"cell_type":"code","source":["import numpy as np\n","def print_decision_boundaries(model, x, y):\n","  dx, dy = 0.1, 0.1\n","  y_grid, x_grid = np.mgrid[slice(np.min(x[:,1]), np.max(x[:,1]) + dy, dy),\n","                  slice(np.min(x[:,0]), np.max(x[:,0]) + dx, dx)]\n","\n","\n","  x_gen = np.concatenate((np.expand_dims(np.reshape(x_grid, (-1)),1),np.expand_dims(np.reshape(y_grid, (-1)),1)), axis=1)\n","  z_gen = model.predict(x_gen).reshape(x_grid.shape)\n","\n","  z_min, z_max = 0, 1\n","\n","  c = plt.pcolor(x_grid, y_grid, z_gen, cmap='RdBu', vmin=z_min, vmax=z_max)\n","  plt.colorbar(c)\n","  plt.plot(x[y==0,0], x[y==0,1], 'r.')\n","  plt.plot(x[y==1,0], x[y==1,1], 'b.')\n","  plt.show()"],"metadata":{"id":"8iSYRgNaL6F-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print_decision_boundaries(model, x_train, y_train)"],"metadata":{"id":"ltsUweGrMPor"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exemple de classification plus \"complexe\""],"metadata":{"id":"mSaSWEnoNxqG"}},{"cell_type":"markdown","source":["Les données de la section précédente sont linéairement séparables : un séparateur linéaire suffit à obtenir des bonnes performances, et un perceptron monocouche est donc bien adapté à la résolution de ce problème.\n","\n","Traitons maintenant un second problème légèrement plus\n","complexe, non linéairement séparable."],"metadata":{"id":"W60rDDAzWTpq"}},{"cell_type":"code","source":["x, y = datasets.make_gaussian_quantiles(n_samples=250, n_features=2, n_classes=2, random_state=1)\n","# Partitionnement des données en apprentissage et test\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=1)\n","\n","# Affichage des données d'apprentissage\n","plt.plot(x_train[y_train==0,0], x_train[y_train==0,1], 'b.', label='Apprentissage, classe 0')\n","plt.plot(x_train[y_train==1,0], x_train[y_train==1,1], 'r.', label='Apprentissage, classe 1')\n","\n","# Affichage des données de test\n","plt.plot(x_test[y_test==0,0], x_test[y_test==0,1], 'b+', label='Test, classe 0')\n","plt.plot(x_test[y_test==1,0], x_test[y_test==1,1], 'r+', label='Test, classe 1')\n","\n","plt.xlabel('x1')\n","plt.ylabel('x2')\n","\n","plt.legend()\n","plt.show()"],"metadata":{"id":"KvhN3uQaN5ji"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Travail à faire** : commencez par tester un perceptron monocouche comme nous l'avons vu précédemment."],"metadata":{"id":"hRMuoA-kDVTR"}},{"cell_type":"code","source":[],"metadata":{"id":"Wu1sp6Os36NU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Un perceptron monocouche ne permet que d'obtenir un séparateur linéaire, ce qui n'est pas suffisant pour apprendre à séparer ces données. La **capacité** du modèle est trop faible, et on est dans une situation de **sous-apprentissage** : la perte d'apprentissage ne diminue pas suffisamment, le modèle n'arrive pas à apprendre correctement l'ensemble d'apprentissage.\n","\n","Pour résoudre ce problème, il faut ajouter (au moins) une couche cachée au modèle. Voici un exemple ci-dessous, où l'on ajoute une couche cachée à 2 neurones."],"metadata":{"id":"1T7wlc0cDnF0"}},{"cell_type":"code","source":["import tensorflow\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","# On définit ici un perceptron multicouche à deux couches cachées.\n","model = Sequential()\n","model.add(Dense(2, activation='relu', input_dim=2)) # La première couche cachée connecte les deux neurones d'entrée à deux neurones\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.summary() # affiche un résumé du modèle\n","\n","# Définition de l'optimiseur\n","sgd = optimizers.SGD(learning_rate=0.1)\n","\n","# Fonction de coût et métrique\n","model.compile(optimizer=sgd,\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Entraînement du modèle\n","history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=50, batch_size=20)\n","\n","# Affichage de l'évolution des métriques\n","plot_training_analysis(history)\n","\n","# Visualisation du résultat\n","print_decision_boundaries(model, x_train, y_train)"],"metadata":{"id":"zZVw7tg9h7Nq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","**Travail à faire** : testez l'entraînement de ce modèle, et au besoin ajustez le nombre de neurones de la couche cachée pour obtenir de bons résultats.\n","\n","**Travail à faire** : testez également de remplacer l'optimiseur SGD par l'optimiseur [Adam](https://keras.io/api/optimizers/adam/)"],"metadata":{"id":"w0LeNFmQh9rn"}},{"cell_type":"code","source":[],"metadata":{"id":"frka3RSh37ZW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sur-apprentissage et régularisation"],"metadata":{"id":"04Lv-gs6tvSh"}},{"cell_type":"markdown","source":["Dans cette section, nous commençons par altérer un peu les données afin qu'elles comportent du \"bruit\" : la frontière de décision entre données bleues et rouges n'est plus si claire."],"metadata":{"id":"knXAouxuh_0W"}},{"cell_type":"code","source":["x, y = datasets.make_gaussian_quantiles(n_samples=250, n_features=2, n_classes=2, random_state=1)\n","x[y==0] = x[y==0]*1.2\n","x[y==1] = x[y==1]*0.8\n","\n","# Partitionnement des données en apprentissage et test\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=1)\n","\n","# Affichage des données d'apprentissage\n","plt.plot(x_train[y_train==0,0], x_train[y_train==0,1], 'b.', label='Apprentissage, classe 0')\n","plt.plot(x_train[y_train==1,0], x_train[y_train==1,1], 'r.', label='Apprentissage, classe 1')\n","\n","# Affichage des données de test\n","plt.plot(x_test[y_test==0,0], x_test[y_test==0,1], 'b+', label='Test, classe 0')\n","plt.plot(x_test[y_test==1,0], x_test[y_test==1,1], 'r+', label='Test, classe 1')\n","\n","plt.xlabel('x1')\n","plt.ylabel('x2')\n","\n","plt.legend()\n","\n","plt.show()"],"metadata":{"id":"EXfOgt3YtxsQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Travail à faire** : Reprenez le modèle à une couche cachée construit précédemment et vérifiez que ce modèle arrive toujours à apprendre une solution satisfaisante."],"metadata":{"id":"gh6xqL5oiRx5"}},{"cell_type":"code","source":[],"metadata":{"id":"vX4aRmuX33-Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["L'avantage de ce modèle à une couche cachée est qu'il reste assez simple : il n'a pas une capacité suffisante pour chercher à expliquer à tout prix la distribution des données d'apprentissage. L'accuracy plafonne vers 75% sur l'ensemble d'apprentissage, ce qui semble évoquer du sous-apprentissage, mais en réalité le modèle est satisfaisant car l'accuracy de validation est elle autour de 70%.\n","\n","\n","Une piste d'amélioration pourrait consister à augmenter la capacité du modèle afin d'obtenir de meilleures performances sur l'ensemble d'apprentissage.\n","\n","**Travail à faire** : Augmentez la capacité du modèle (nombre de couches cachées, nombre de neurones par couche) de sorte d'obtenir une accuracy d'apprentissage d'environ 90%. Qu'observez-vous sur l'accuracy de validation ?"],"metadata":{"id":"fS-7aeU1ieBS"}},{"cell_type":"code","source":[],"metadata":{"id":"kscC_32r38_r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["On observe du **sur-apprentissage** ! L'accuracy d'entrainement est élevée, mais l'accuracy de validation est faible. On doit alors chercher à limiter le sur-apprentissage, et pour ce faire on peut utiliser des techniques de régularisation. Une des techniques vues en cours consiste à appliquer une pénalité sur la norme des paramètres du modèle. Ainsi, plutôt que de minimiser une fonction objectif $J(\\theta)$ classique :\n","$$ J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} \\text{loss}(y^{(i)}, \\hat{y}^{(i)}) $$\n","\n","on minimise plutôt :\n","$$ J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} \\text{loss}(y^{(i)}, \\hat{y}^{(i)}) + \\lambda \\sum_{j=1}^M \\theta_j^2 $$\n","\n","où $M$ désigne le nombre de paramètres (poids synaptiques et biais) du réseau, $\\theta$ désigne les paramètres du réseau.\n","\n","En pratique il suffit d'appliquer cette pénalisation aux poids synaptiques seuls (car ils sont beaucoup plus nombreux).\n","\n","La librairie Keras permet d'appliquer de la régularisation à n'importe quelle couche, [comme décrit sur cette page.](https://keras.io/api/layers/regularizers/)\n","\n","**Travail à faire**: Reprenez votre réseau précédent (avec lequel on obtenait du sur-apprentissage) et intégrez de la régularisation pour limiter le sur-apprentissage."],"metadata":{"id":"bGKcZdKljf0R"}},{"cell_type":"code","source":[],"metadata":{"id":"_sU9kPovu4CC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Problème à 3 classes\n","\n","Tous les problèmes que nous avons abordés jusqu'à maintenant étaient des problèmes de classification binaire (fonction d'activation sigmoide).\n","\n","Dans cette section nous définissons un jeu de données comportant trois classes."],"metadata":{"id":"9pS_yuIawIun"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn import datasets\n","import matplotlib.pyplot as plt\n","\n","# Génération des données\n","x, y = datasets.make_blobs(n_samples=500, n_features=2, centers=3, center_box=(- 5, 5), random_state=3)\n","# Partitionnement des données en apprentissage et test\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=1)\n","\n","# Affichage des données d'apprentissage\n","plt.plot(x_train[y_train==0,0], x_train[y_train==0,1], 'b.', label='Apprentissage, classe 0')\n","plt.plot(x_train[y_train==1,0], x_train[y_train==1,1], 'r.', label='Apprentissage, classe 1')\n","plt.plot(x_train[y_train==2,0], x_train[y_train==2,1], 'g.', label='Apprentissage, classe 2')\n","\n","plt.xlabel('x1')\n","plt.ylabel('x2')\n","\n","# Affichage des données de test\n","plt.plot(x_test[y_test==0,0], x_test[y_test==0,1], 'b+', label='Test, classe 0')\n","plt.plot(x_test[y_test==1,0], x_test[y_test==1,1], 'r+', label='Test, classe 1')\n","plt.plot(x_test[y_test==2,0], x_test[y_test==2,1], 'g+', label='Test, classe 1')\n","\n","plt.legend()\n","plt.show()"],"metadata":{"id":"bFoSzpMYwIm6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["La fonction ci-dessous permet d'afficher les frontières de décision apprises par le modèle."],"metadata":{"id":"EIgCrAzQpJGN"}},{"cell_type":"code","source":["def print_decision_boundaries_3classes(model, x, y):\n","    # Adjust step sizes to match the input data\n","    dx = (np.max(x[:, 0]) - np.min(x[:, 0])) / 100\n","    dy = (np.max(x[:, 1]) - np.min(x[:, 1])) / 100\n","\n","    # Create a grid with adjusted step sizes\n","    y_grid, x_grid = np.mgrid[slice(np.min(x[:, 1]), np.max(x[:, 1]) + dy, dy),\n","                               slice(np.min(x[:, 0]), np.max(x[:, 0]) + dx, dx)]\n","\n","    # Generate points for prediction\n","    x_gen = np.stack((x_grid.ravel(), y_grid.ravel()), axis=-1)\n","    y_pred_gen = np.argmax(model.predict(x_gen), axis=1)\n","    z_gen = y_pred_gen.reshape(x_grid.shape)\n","\n","    # Define color range\n","    z_min, z_max = 0, 2\n","\n","    # Plot decision boundaries\n","    c = plt.pcolor(x_grid, y_grid, z_gen, cmap='brg', vmin=z_min, vmax=z_max, alpha=0.5)\n","    plt.colorbar(c)\n","\n","    # Plot data points for each class\n","    plt.plot(x[y == 0, 0], x[y == 0, 1], 'bo', label='Class 0')\n","    plt.plot(x[y == 1, 0], x[y == 1, 1], 'ro', label='Class 1')\n","    plt.plot(x[y == 2, 0], x[y == 2, 1], 'go', label='Class 2')\n","\n","    # Add legend\n","    plt.legend()\n","    plt.show()"],"metadata":{"id":"8uVHVbjrTYPQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Lorsque les données appartiennent à plus de deux classes, il est nécessaire de formater les labels sous la forme de *one-hot vectors*, c'est-à-dire de de vecteurs constitués de 0 et d'un 1 à l'indice de la classe correspondante.\n","\n","Ainsi, pour un problème à 3 classes :\n","- si $x^{(i)}$ appartient à la classe 0, $y^{(i)} = [1, 0, 0]^T$\n","- si $x^{(i)}$ appartient à la classe 1, $y^{(i)} = [0, 1, 0]^T$\n","- si $x^{(i)}$ appartient à la classe 2, $y^{(i)} = [0, 0, 1]^T$\n","\n","Dans le jeu de données introduit plus haut, les labels $y \\in \\{0, 1, 2\\}$. Pour les convertir au format *one-hot*, vous pouvez utiliser la fonction [to_categorical](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical).\n","\n","**Travail à faire**: Construisez un perceptron monocouche capable de résoudre ce problème de classification à 3 classes.\n","\n","**ATTENTION**: Pour les problèmes à plus de deux classes, vous devez utiliser une fonction d'activation adaptée (*softmax*) ainsi que la fonction de coût associée."],"metadata":{"id":"_SWJbM5OpXWR"}},{"cell_type":"code","source":[],"metadata":{"id":"t-ji8W1uxG5U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Application à une base de données réelle\n","\n","Pour mettre en application les notions vues précédemment, téléchargez les données suivantes."],"metadata":{"id":"SFZHmkdtQT6z"}},{"cell_type":"code","source":["!wget acarlier.fr/tp/HA_dataset.csv"],"metadata":{"id":"aziXifjYQXHN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import csv\n","from sklearn.model_selection import train_test_split\n","\n","x = []\n","y = []\n","\n","# Lecture du fichier CSV et des données\n","with open('HA_dataset.csv', 'r') as file:\n","    reader = csv.reader(file)\n","    headers = next(reader)  # Ligne d'en-tête\n","    for row in reader:\n","        x.append([float(value) for value in row[:-1]])  # Toutes les colonnes à l'exception de la dernières sont les variables d'entrée\n","        y.append(int(row[-1]))  # La dernière colonne constitue le label à prédire\n","\n","x = np.array(x)\n","y = np.array(y)\n","\n","# Séparation des données en jeux d'apprentissage et de test\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","\n","# Affichage de la dimension des données\n","print(\"Dimension de x_train:\", x_train.shape)\n","print(\"Dimension de y_train:\", y_train.shape)\n","print(\"Dimension de x_test:\", x_test.shape)\n","print(\"Dimension de y_test:\", y_test.shape)"],"metadata":{"id":"ZiJzD4CsUzNu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ces données sont issues de https://archive.ics.uci.edu/dataset/45/heart+disease et regroupent des caractéristiques de patients risquant ($y=1$) de développer une attaque cardiaque. Voici les différentes caractéristiques sur lesquelles votre modèle pourra s'appuyer pour établir un pronostic :\n","- *age* : Âge du patient\n","- *sex* : Sexe du patient (0-1)\n","- *exang* : Angine induite par l'exercice (1 = oui ; 0 = non)\n","- *ca* : Nombre de vaisseaux principaux (0-3)\n","- *cp* : Type de douleur thoracique (1-4)\n","- *trestbps* : Pression artérielle au repos (en mm de mercure)\n","- *chol* : Cholestérol en mg/dl récupéré via un capteur BMI\n","- *fbs* : (glycémie à jeun > 120 mg/dl) (1 = vrai ; 0 = faux)\n","- *restecg* : Résultats électrocardiographiques au repos (0-2)\n","- *thalach* : Fréquence cardiaque maximale atteinte\n","- *oldpeak* : Dépression du segment ST induite par l'exercice par rapport au repos\n","- *slope*, *thal* : signification non renseignée\n"],"metadata":{"id":"XtY5X4RMsp7_"}},{"cell_type":"markdown","source":["**Travail à faire** : Construisez un classifieur permettant d'obtenir les meilleurs résultats possibles sur l'ensemble de test de ce jeu de données.\n","\n","\n","**N.B.** Prenez garde à l'échelle des données d'entrée ! Le code ci-dessous vous permettra de les répresenter sous forme graphique. Il peut être intéressant de les normaliser..."],"metadata":{"id":"14PiBdxJz8Tk"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","\n","combined_data = np.column_stack((x_train, y_train))\n","combined_data_transposed = list(map(list, zip(*combined_data)))\n","\n","# Création d'un \"diagramme à moustache\"\n","plt.figure(figsize=(10, 6))\n","plt.boxplot(combined_data_transposed, labels=headers[:-1] + ['target'])\n","plt.xticks(rotation=45)\n","plt.title('Distribution des valeurs de chacune des variables')\n","plt.xlabel('Variables')\n","plt.ylabel('Valeurs')\n","plt.yscale('log')  # Echelle logarithmique\n","\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"pjR5D53RWK5a"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}