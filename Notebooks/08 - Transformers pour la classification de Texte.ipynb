{"cells":[{"cell_type":"markdown","metadata":{"id":"Q4gemxU6eYk-"},"source":["# Transformers pour la classification de texte\n","\n","L'objectif de ce TP est d'implémenter une version simplifiée d'un Transformer pour résoudre un problème de classification de texte.\n","\n","Nous utiliserons comme exemple illustratif une base de données présente dans la librairie ```Keras``` consistant en des critiques de films postées sur le site IMDB, accompagnées d'une note qui a été binarisée pour révéler le caractère positif, ou négatif, de la critique."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ypGxrmyTeYlE"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras"]},{"cell_type":"markdown","metadata":{"id":"1jBnJa2VeYlF"},"source":["## Implémentation d'un bloc de base de Transformer\n","\n","\n","<center><img src=\"https://drive.google.com/uc?id=1leAVyTZJ2gZ26CFoauMtmu7T4Jo-Crc_\" width=200> </center>\n","<caption><center> Figure 1: Schéma de l'architecture de BERT</center></caption>\n","\n","La figure ci-dessus présente l'architecture de BERT. Le bloc de base d'un Transformer est composé d'un bloc de *Self-Attention*, d'une couche de ```Layer Normalization```, d'une couche dense et enfin d'une nouvelle couche de ```Layer Normalization```. L'idée de la couche de ```Layer Normalization```est de faciliter le processus d'apprentissage en renormalisant les activations après chaque couche du réseau afin d'éviter que les valeurs ne s'écartent trop de 0.\n","Prêtez également bien attention aux **couches résiduelles**.\n","\n","Pour implémenter la *Self-Attention*, vous pouvez utiliser la fonction ```Multi-Head Attention``` (à vous de regarder quels en sont les paramètres dans la documentation).\n","\n","**Rappel**: Une couche d'Attention *Multi-Head*  se présente sous la forme ci-dessous à gauche, avec le mécanisme d'attention détaillé à droite :\n","\n","\n","<center>\n","\n","<img src=\"https://drive.google.com/uc?id=1UTozEHtsZ3xy61XJqn_Eug-7mn7bFp9m\">\n","<img src=\"https://drive.google.com/uc?id=1aTttpp1OOasVVZAi3lWwosh68VnBjQnz\">\n","</center>\n","\n","**D'après vous, combien de paramètres comporte une couche d'attention à 2 têtes, pour un *Embedding* de dimension 32 ?**\n","\n","\n","Pour implémenter un bloc Transformer, je vous propose dans ce TP d'adopter une notation ressemblant beaucoup à Pytorch. Nous allons instancier une classe *Layer*, et notamment les deux fonctions :    \n","-  ```__init__()```, un constructeur dans lequel nous initialisons toutes les couches que nous allons utiliser\n","-  ```call()``` appelé pour la prédiction et qui décrit comment les différentes couches sont appliquées aux entrées."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VuK3YnbLeYlF"},"outputs":[],"source":["class TransformerBlock(layers.Layer):\n","    # embed_dim désigne la dimension des embeddings maintenus à travers les différentes couches,\n","    # et num_heads le nombre de têtes de la couche d'attention.\n","    # DANS CETTE FONCTION, ON NE FAIT QUE DEFINIR LES COUCHES\n","    def __init__(self, embed_dim, num_heads):\n","        super().__init__()\n","        # Définition des différentes couches qui composent le bloc\n","        # Couche d'attention\n","        self.att =\n","        # Première couche de Layer Normalization\n","        self.layernorm1 =\n","        # Couche Dense (Feed-Forward)\n","        self.ffn =\n","        # Deuxième couche de normalisation\n","        self.layernorm2 =\n","\n","    # DANS CETTE FONCTION, ON APPELLE EXPLICITEMENT LES COUCHES DEFINIES DANS __init__\n","    # ON PROPAGE DONC LES ENTREES inputs A TRAVERS LES DIFFERENTES COUCHES POUR OBTENIR\n","    # LA SORTIE\n","    def call(self, inputs):\n","        # Application des couches successives aux entrées\n","        return ..."]},{"cell_type":"markdown","metadata":{"id":"OA9aXUkleYlG"},"source":["## Implémentation de la double couche d'Embedding\n","\n","La séquence d'entrée est convertie en *Embedding* de dimension ```embed_dim```.\n","L'*Embedding* final est constitué de la somme de deux *Embedding*, le premier encodant un mot, et le second encodant la position du mot dans la séquence.\n","\n","La couche d'*Embedding* de Keras (```layers.Embedding```) est une sorte de table associant à un indice en entrée un vecteur de dimension ```embed_dim```. Chaque coefficient de cette table est en fait un paramètre apprenable.\n","\n","**D'après vous combien de paramètres contiendrait une couche d'*Embedding* associant un vecteur de dimension 32 à chacun des 20000 mots les plus courants du vocabulaire extrait de la base de données que nous allons utiliser ?\n","Et combien pour l'*Embedding* qui associe un vecteur de dimension 32 à chaque position d'un séquence de longueur ```maxlen``` ?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RPGCy1t8eYlG"},"outputs":[],"source":["class TokenAndPositionEmbedding(layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim):\n","        super().__init__()\n","        # Définition des différentes couches qui composent le bloc Embedding\n","        # Embedding de mot\n","        self.token_emb =\n","        # Embedding de position\n","        self.pos_emb =\n","\n","    def call(self, x):\n","        # Calcul de l'embedding à partir de l'entrée x\n","        # ATTENTION : UTILISER UNIQUEMENT DES FONCTIONS TF POUR CETTE PARTIE\n","        # Récupération de la longueur de la séquence\n","        maxlen =\n","        # Création d'un vecteur [0, 1, ..., maxlen] des positions associées aux\n","        # mots de la séquence (fonction tf.range)\n","        positions =\n","        # Calcul des embeddings de position\n","        positions_emb = ...\n","        # Calcul des embeddings de mot\n","        words_emb = ...\n","        return"]},{"cell_type":"markdown","metadata":{"id":"4ijFa-H_eYlG"},"source":["## Préparation de la base de données"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3EciJaw4eYlG"},"outputs":[],"source":["# Taille du vocabulaire considéré (on ne conserve que les 20000 mots les plus courants)\n","vocab_size = 20000\n","# Taille maximale de la séquence considérée (on ne conserve que les 200 premiers mots de chaque commentaire)\n","maxlen = 200\n","\n","# Chargement des données de la base IMDB\n","(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n","\n","print(len(x_train), \"séquences d'apprentissage\")\n","print(len(x_val), \"séquences de validation\")\n","\n","# Padding des séquences : ajout de \"0\" pour compléter les séquences trop courtes\n","x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"]},{"cell_type":"markdown","metadata":{"id":"7CkmQNEReYlH"},"source":["## Création du modèle\n","\n","Pour assembler le modèle final, il faut, partant d'une séquence de longueur ```maxlen```, calculer les Embedding puis les fournir en entrée d'une série de blocs Transformer. Pour ce TP, **commencez par ne mettre qu'un seul bloc Transformer**. Vous pourrez en ajouter plus tard si vous le souhaitez.\n","\n","Pour construire la tête de projection du réseau, vous pouvez moyenner les activations en sortie du bloc Transformer par élément de la séquence grâce à un *Global Average Pooling* (1D !), à relier à une couche dense (par exemple comportant 20 neurones) et enfin à la couche de sortie du réseau."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FiRdlO_SeYlH"},"outputs":[],"source":["embed_dim = 32  # Dimension de l'embedding pour chaque mot\n","num_heads = 2  # Nombre de têtes d'attention\n","\n","# A COMPLETER\n","inputs = layers.Input(shape=(...))\n","...\n","...\n","outputs = ...\n","\n","model = keras.Model(inputs=inputs, outputs=outputs)\n","model.summary()"]},{"cell_type":"markdown","source":["Enfin vous pouvez lancer l'apprentissage, avec par exemple l'optimiseur Adam. Inutile de lancer de trop nombreuses *epochs*, le réseau sur-apprend très vite !"],"metadata":{"id":"6TZq2_8kopTL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rfeEvasteYlH"},"outputs":[],"source":["# A COMPLETER\n","model.compile(\n","    optimizer=\"adam\", loss=\"...\", metrics=[\"...\"]\n",")\n","history = model.fit(\n","    x_train, y_train, batch_size=32, epochs=5, validation_data=(x_val, y_val)\n",")"]},{"cell_type":"markdown","source":["# Utilisation d'un modèle pré-entraîné"],"metadata":{"id":"C-QKH6IHqyFV"}},{"cell_type":"markdown","source":["L'implémentation d'un Transformer *from scratch* comme dans la section précédente n'est en fait pas réellement recommandée. Il est souvent bien plus pertinent d'utiliser un modèle pré-entraîné et de faire du transfert d'apprentissage.\n","\n","Dans cette 2nde partie, nous allons utiliser des modèles fournis par HuggingFace pour appréhender les différentes parties d'un système de classification de texte, puis pour résoudre le problème de la partie précédente d'une manière plus satisfaisante."],"metadata":{"id":"wpQ0NIDqq0hq"}},{"cell_type":"markdown","source":["## Tokenization\n","\n","La tokenization désigne l'opération de découpage du texte initial en une séquence de *tokens*, c'est-à-dire d'unités indivisibles de texte. Dans la partie précédente, nous avons implicitement utilisé une simple Tokenization au niveau mot.\n","\n","Pour jouer un peu avec ce concept, chargeons le Tokenizer du modèle CAMEMBERT. CAMEMBERT est une version de BERT entraînée sur des données uniquement en français. Le Tokenizer a donc été préparé uniquement pour des mots en français.\n","\n","\n"],"metadata":{"id":"Is5v4aOqq6O2"}},{"cell_type":"code","source":["from transformers import CamembertTokenizer\n","\n","# Charger le tokenizer CamemBERT\n","tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n","\n","# Phrase en français\n","sentence = \"J'apprécie les fruits au sirop.\"\n","\n","# Tokenisation de la phrase\n","encoded = tokenizer(sentence, return_tensors=\"tf\")\n","\n","# Afficher les IDs des tokens\n","token_ids = encoded[\"input_ids\"].numpy()[0]  # Convertir en numpy pour affichage\n","print(\"IDs des tokens :\", token_ids)\n","\n","# Associer chaque ID au mot correspondant\n","tokens = [tokenizer.decode([token_id]) for token_id in token_ids]\n","\n","# Afficher les tokens avec leur ID\n","print(\"\\nCorrespondance ID -> Token :\")\n","for token_id, token in zip(token_ids, tokens):\n","    print(f\"{token_id} -> {token}\")\n"],"metadata":{"id":"YolvDD7bWF7P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sur cet exemple, on a l'impression que la Tokenization de CAMEMBERT a été également réalisée au niveau mot. On repère les tokens spéciaux \\<s\\> (pour démarrer une phrase) et \\</s\\> (pour la terminer). *Notez ici pour la suite que cette phrase compte 10 tokens.*\n","\n","En réalité, le Tokenizer est hybride : il décompose les phrases en mots si ceux-ci sont très communs, mais aussi en syllabes ou même en simples caractères.\n","\n","**Travail à faire : Essayez de trouver des exemples de phrases pour faire apparaître des tokens qui ne sont pas des mots.**"],"metadata":{"id":"MFcHT1xErATH"}},{"cell_type":"markdown","source":["## Complétion de texte masqué par CAMEMBERT\n","\n","Les modèles de type BERT, comme CAMEMBERT, sont pré-entraînés sur de vastes corpus de textes de manière non supervisée. La méthode d'entraînement la plus commune est de leur faire compléter des morceaux de phrase qui ont été masqués à l'aide d'un token spécial.\n","\n","\n","Il est intéressant de tester la capacité du modèle à compléter un texte masqué en appliquant la procédure suivante :"],"metadata":{"id":"UBslfoBjrGke"}},{"cell_type":"code","source":["from transformers import TFCamembertForMaskedLM\n","import tensorflow as tf\n","\n","# Chargement du modèle Camembert\n","model = TFCamembertForMaskedLM.from_pretrained(\"camembert-base\")\n","\n","# Phrase en français masquée\n","sentence = \"J'apprécie les <mask> au sirop.\"\n","\n","# Tokenisation de la phrase\n","encoded = tokenizer(sentence, return_tensors=\"tf\")\n","\n","# Prédiction du modèle sur la phrase\n","outputs = model(encoded)\n","\n","# Affichage de la dimension du tenseur prédit\n","print(outputs.logits.shape)\n"],"metadata":{"id":"hvcj0R3wrA3_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Le tenseur prédit par le modèle CAMEMBERT sur la phrase d'exemple est de taille $1 \\times 10 \\times 32005$ :    \n","- le 1 correspond à la taille du batch (ici simplement 1)\n","- le 10 correspond au nombre de tokens de la séquence, c'est la valeur que nous avions trouvée précédemment. Le symbole \\<mask\\> compte pour un token.\n","- La veleur 32005 correspond au nombre de tokens de la tokenization de CAMEMBERT. On obtient ainsi une distribution de probabilité sur tous ces tokens.\n","\n","Pour prédire le mot masqué, il suffit de retrouver parmi les 10 tokens, l'indice du token \\<mask\\> et de regarder la distribution de probabilité prédite par le modèle. On peut ainsi afficher les tokens ayant la probabilité la plus élevée de compléter le trou dans la phrase."],"metadata":{"id":"1sL-WUrErMaU"}},{"cell_type":"code","source":["### A COMPLETER\n","# Récupérer l'index du token masqué\n","mask_index = tf.where(encoded[\"input_ids\"] == tokenizer.mask_token_id).numpy().flatten()[1]\n","\n","# Obtenir les logits des prédictions pour le token masqué\n","logits = ...\n","\n","# Il faut appliquer la fonction softmax pour obtenir une distribution de probabilité\n","probs = ...\n","\n","# On collecte les 5 prédictions les plus probables, ainsi que leur probabilité\n","top_5 = tf.math.top_k(probs, k=5)\n","\n","# Associer chaque token ID à son mot et sa probabilité\n","predictions = [(tokenizer.decode([idx]).strip(), prob) for idx, prob in zip(top_5.indices.numpy(), top_5.values.numpy())]\n","\n","# Afficher les mots prédits avec leurs probabilités\n","print(\"Mots prédits avec probabilités :\")\n","for word, prob in predictions:\n","    print(f\"{word}: {prob:.4f}\")"],"metadata":{"id":"9Tb9LYlxrM3T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Transfert d'apprentissage d'un modèle BERT pré-entraîné\n","\n","Revenons maintenant au problème initial de classification de reviews IMDB.\n","\n","Nous devons d'abord résoudre un petit problème : la base de données fournie par Keras ne contient que les identifiants des mots écrits dans les reviews, afin de s'abstraire des problématiques de tokenization.\n","\n","Pour pouvoir utiliser le Tokenizer du modèle que nous allons choisir, il nous faut disposer des phrases originelles. Nous allons adopter une méthode un peu imparfaite et nous allons reconstruire les reviews en utilisant le dictionnaire (index vers mot) fourni par Keras. Malheureusement, comme nous avons choisi de ne conserver que les 20 000 mots les plus courants, il manquera certains mots !"],"metadata":{"id":"zmnL2saOrXCl"}},{"cell_type":"code","source":["# Récupération du dictionnaire mot->index\n","word_index = keras.datasets.imdb.get_word_index()\n","\n","# Insertion des tokens rajoutés par Keras\n","word_index = {word: (index + 3) for word, index in word_index.items()}\n","word_index[\"<PAD>\"] = 0   # token de padding\n","word_index[\"<START>\"] = 1  # token de début de review\n","word_index[\"<UNK>\"] = 2    # Token pour les mots inconnus\n","word_index[\"<UNUSED>\"] = 3\n","print(word_index)\n","\n","# Inversion du dictionnaire : index->mot\n","reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n","\n","print(reverse_word_index)"],"metadata":{"id":"ulrRGDRDrXZq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nous pouvons maintenant, à l'aide du dictionnaire, retrouver les phrases initiales de la base de données :\n","\n"],"metadata":{"id":"engyVYWgr6PC"}},{"cell_type":"code","source":["def indices_to_text(indices):\n","    return \" \".join(reverse_word_index.get(i, \"<UNK>\") for i in indices)\n","\n","# Affichage de la première review de la base d'entraînement\n","decoded_review = indices_to_text(x_train[0])\n","\n","print(decoded_review)"],"metadata":{"id":"JafUY8R0sBTU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["On observe en effet qu'il manque des mots !\n","\n","Si nous chargeons maintenant le Tokenizer du modèle BERT, nous pouvons ainsi observer la version tokenizée de la première review de la base d'apprentissage. Vous comprenez donc que le processus que nous venons d'appliquer nous a permis de retrouver la base initiale depuis la Tokenization de Keras, afin de pouvoir préparer cette base à la classification par BERT en lui appliquant le Tokenizer correspondant !"],"metadata":{"id":"WhGYmZQHsFKU"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","# Chargement du Tokenizer du modèle ALBERT\n","tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")\n","\n","# Tokenization de la première review du dataset\n","tokenizer(decoded_review, return_tensors=\"np\", padding=True)"],"metadata":{"id":"fcRsQWUWsZmN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Maintenant que cette preuve de concept fonctionne nous pouvons appliquer le même processus à l'ensemble de la base d'apprentissage et de la base de validation :"],"metadata":{"id":"VuaOsz7gslaD"}},{"cell_type":"code","source":["# On convertit maintenant l'ensemble des reviews en texte\n","texts_train = [indices_to_text(review) for review in x_train]\n","texts_val = [indices_to_text(review) for review in x_val]\n","\n","# On applique le Tokenizer du modèle ALBERT sur toutes les reviews\n","x_train_bert = tokenizer(texts_train, return_tensors=\"np\", padding=True)\n","x_val_bert = tokenizer(texts_val, return_tensors=\"np\", padding=True)\n","\n","# Enfin, on prépare les tenseurs pour l'apprentissage en les passant au bon format tensorflow\n","x_train_bert_tensors = {key: tf.convert_to_tensor(value) for key, value in x_train_bert.items()}\n","x_val_bert_tensors = {key: tf.convert_to_tensor(value) for key, value in x_val_bert.items()}"],"metadata":{"id":"sSCWzZwusrws"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nous pouvons maintenant lancer le transfert d'apprentissage du modèle ALBERT, qui est une version plus compacte de BERT. Attention cette dernière cellule est longue ! Cette partie est plus pour vous donner un exemple de fine-tuning mais de nombreux tutorials existent pour l'accélérer (par exemple, en utilisant des modèles quantifiés)"],"metadata":{"id":"AOMfNVV3tLur"}},{"cell_type":"code","source":["from transformers import TFAutoModelForSequenceClassification\n","from transformers import AdamWeightDecay\n","\n","# Load and compile our model\n","model = TFAutoModelForSequenceClassification.from_pretrained(\"albert-base-v2\", num_labels=2)\n","model.summary()\n","# Lower learning rates are often better for fine-tuning transformers\n","optimizer = AdamWeightDecay(learning_rate=3e-5)\n","model.compile(optimizer=optimizer)  # No loss argument!\n","\n","model.fit(x_train_bert_tensors, y_train, batch_size=8, validation_data=(x_val_bert_tensors, y_val))"],"metadata":{"id":"qn5opazpteMP"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/nlp/ipynb/text_classification_with_transformer.ipynb","timestamp":1674139249077}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"}},"nbformat":4,"nbformat_minor":0}